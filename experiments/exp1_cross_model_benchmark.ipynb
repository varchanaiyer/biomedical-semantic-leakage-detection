{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a001",
   "metadata": {},
   "source": [
    "# Experiment 1: Cross-Model Semantic Leakage Benchmark\n",
    "\n",
    "**Research Question:** Which LLM produces the most logically consistent biomedical Chain-of-Thought reasoning?\n",
    "\n",
    "**What we measure:**\n",
    "- Contradiction rate between adjacent reasoning steps (per model)\n",
    "- How contradiction rate grows with reasoning depth (step index)\n",
    "- UMLS concept validity rate and its correlation with logical consistency\n",
    "- Guard signal distribution across models\n",
    "\n",
    "**Pipeline:** Question â†’ LLM CoT â†’ UMLS Concept Extraction â†’ Hybrid NLI Entailment â†’ Analysis\n",
    "\n",
    "Results are cached to `results/exp1_*.json` so re-running the notebook skips expensive API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-colab-exp1",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP: Clone repo, install deps, set API keys\n# Run this cell first â€” works in Colab and local Jupyter\n# ============================================================\nimport os, sys\nfrom pathlib import Path\n\n# â”€â”€ 1. Clone or update the repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nREPO_URL  = 'https://github.com/varchanaiyer/biomedical-semantic-leakage-detection'\nREPO_DIR  = 'biomedical-semantic-leakage-detection'\n\nif not Path(REPO_DIR).exists():\n    os.system(f'git clone {REPO_URL}')\nelse:\n    os.system(f'git -C {REPO_DIR} pull --quiet')\n\n# â”€â”€ 2. Add project root to path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n_cwd = Path(os.getcwd())\nif (_cwd / REPO_DIR / 'utils').exists():\n    PROJECT_ROOT = str(_cwd / REPO_DIR)\nelif (_cwd / 'utils').exists():\n    PROJECT_ROOT = str(_cwd)\nelif (_cwd.parent / 'utils').exists():\n    PROJECT_ROOT = str(_cwd.parent)\nelse:\n    PROJECT_ROOT = str(_cwd / REPO_DIR)  # fallback\n\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\nos.chdir(PROJECT_ROOT)\nprint(f'PROJECT_ROOT: {PROJECT_ROOT}')\n\n# â”€â”€ 3. Install dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nos.system('pip install openai numpy pandas scipy scikit-learn matplotlib seaborn requests jupyter ipywidgets python-dotenv --quiet')\n\n# â”€â”€ 4. Set API keys directly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nos.environ[\"UMLS_API_KEY\"]       = \"YOUR_UMLS_API_KEY_HERE\"\nos.environ[\"OPENROUTER_API_KEY\"] = \"YOUR_OPENROUTER_API_KEY_HERE\"\n\nprint('Setup complete. API keys configured:', {\n    k: ('set' if os.environ.get(k) else 'NOT SET')\n    for k in ['OPENROUTER_API_KEY', 'UMLS_API_KEY']\n})"
  },
  {
   "cell_type": "code",
   "id": "setup-colab-exp1-keys",
   "metadata": {},
   "source": [
    "# â”€â”€ OpenRouter API Key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os, importlib.util\nfrom IPython.display import display, clear_output, HTML\n\n_HAS_WIDGETS = importlib.util.find_spec(\"ipywidgets\") is not None\n\nif _HAS_WIDGETS:\n    import ipywidgets as widgets\n\n    _key_box = widgets.Password(\n        placeholder=\"sk-or-v1-â€¦  (get yours free at openrouter.ai)\",\n        layout=widgets.Layout(width=\"520px\"),\n    )\n    _btn = widgets.Button(\n        description=\"Set Key\", button_style=\"primary\",\n        icon=\"check\", layout=widgets.Layout(width=\"110px\"),\n    )\n    _out = widgets.Output()\n\n    def _apply(_b):\n        with _out:\n            clear_output()\n            key = _key_box.value.strip()\n            if key:\n                os.environ[\"OPENROUTER_API_KEY\"] = key\n                print(f\"  âœ“ OpenRouter key set ({len(key)} chars)\")\n            else:\n                print(\"  âœ— Paste your OpenRouter key above, then click Set Key.\")\n\n    _btn.on_click(_apply)\n    display(HTML(\"<b>ğŸ”‘ OpenRouter API Key</b>\"))\n    display(widgets.HBox([_key_box, _btn]))\n    display(_out)\n    display(HTML(\n        \"<small>Get a free key at \"\n        \"<a href=\\\"https://openrouter.ai\\\" target=\\\"_blank\\\">openrouter.ai</a>\"\n        \" â€” the notebooks will automatically run across all configured models.</small>\"\n    ))\nelse:\n    os.environ.setdefault(\"OPENROUTER_API_KEY\", \"\")\n    print(\"ipywidgets not found â€” set key with:\")\n    print(\"  os.environ[\\\"OPENROUTER_API_KEY\\\"] = \\\"sk-or-v1-...\\\"\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002",
   "metadata": {},
   "outputs": [],
   "source": "import sys, os, json, time, pickle\nfrom pathlib import Path\n\n# Add project root to path\n# Project root (setup cell already set CWD and sys.path; this is a fallback for local use)\n_cwd = Path(os.getcwd())\nPROJECT_ROOT = _cwd if (_cwd / 'utils').exists() else _cwd.parent\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\nRESULTS_DIR = PROJECT_ROOT / 'experiments' / 'results'\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Use heuristic NLI â€” fast, no HuggingFace model download needed.\n# Set to False only for final camera-ready runs (requires ~800 MB download).\nUSE_HEURISTIC_NLI = True\nif USE_HEURISTIC_NLI:\n    os.environ['FORCE_HEURISTIC_NLI'] = '1'\n\nprint(f'Project root: {PROJECT_ROOT}')\nprint(f'Results dir:  {RESULTS_DIR.resolve()}')\nprint(f'Heuristic NLI: {USE_HEURISTIC_NLI}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.cot_generator import generate as generate_cot\n",
    "from utils.concept_extractor import extract_concepts\n",
    "from utils.hybrid_checker import build_entailment_records\n",
    "from utils.guards import derive_guards, GuardConfig\n",
    "from utils.umls_api_linker import is_configured as umls_configured\n",
    "\n",
    "print('All modules imported successfully.')\n",
    "print(f'UMLS configured: {umls_configured()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ 15 Diverse Biomedical Questions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Representative subset covering drugs, diseases, mechanisms, diagnostics, treatments.\n# 15 questions Ã— 2 models = 30 API calls (fast enough for iterative runs).\n# Scale to 40 questions Ã— 4 models for camera-ready by uncommenting extras below.\n\nQUESTIONS = [\n    # Drug mechanisms\n    \"Does aspirin reduce the risk of myocardial infarction in patients with cardiovascular disease?\",\n    \"What is the mechanism by which metformin lowers blood glucose in type 2 diabetes?\",\n    \"How do statins reduce LDL cholesterol levels and cardiovascular risk?\",\n    # Disease processes\n    \"What is the pathophysiology of atherosclerosis leading to coronary artery disease?\",\n    \"How does type 2 diabetes lead to peripheral neuropathy?\",\n    \"What is the mechanism of hypertension-induced end-organ damage?\",\n    # Diagnostics\n    \"What are the diagnostic criteria for sepsis and how should it be managed?\",\n    \"What biomarkers are used to diagnose acute myocardial infarction?\",\n    \"How is systemic lupus erythematosus diagnosed using laboratory tests?\",\n    # Treatments\n    \"What is the first-line treatment for community-acquired pneumonia in outpatients?\",\n    \"How is atrial fibrillation managed to prevent thromboembolic complications?\",\n    \"What is the role of immunotherapy in treating non-small cell lung cancer?\",\n    # Drug interactions / multi-step\n    \"What are the risks of combining NSAIDs with anticoagulants?\",\n    \"What is the connection between obesity, insulin resistance, and type 2 diabetes?\",\n    \"How does the renin-angiotensin-aldosterone system contribute to hypertension?\",\n]\n\nprint(f'Total questions: {len(QUESTIONS)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Pipeline Runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nGUARD_CFG = GuardConfig()\n\ndef run_full_pipeline(question: str, prefer: str = 'openrouter',\n                      model: str = None,\n                      scispacy_when: str = 'never', top_k: int = 3) -> dict:\n    \"\"\"Run the full pipeline on a single question.\"\"\"\n    import time\n    t0 = time.time()\n\n    # Step 1: CoT generation\n    cot      = generate_cot(question, prefer=prefer, model=model)\n    steps    = cot.get('steps', [])\n    provider = cot.get('provider', 'unknown')\n    model_id = cot.get('model', model or 'unknown')\n\n    # Step 2: Concept extraction (UMLS)\n    concepts = extract_concepts(steps, scispacy_when=scispacy_when, top_k=top_k)\n\n    # Step 3: Hybrid NLI entailment\n    pairs = build_entailment_records(steps, concepts)\n\n    # Step 4: Guard signals + UMLS relation metadata\n    guarded_pairs = []\n    for p in pairs:\n        i, j = p['step_pair']\n        si = steps[i] if i < len(steps) else ''\n        sj = steps[j] if j < len(steps) else ''\n\n        meta = p.get('meta', {})\n        guards = derive_guards(\n            premise       = si,\n            hypothesis    = sj,\n            probs         = p['probs'],\n            reverse_probs = None,\n            relation_violation = bool(meta.get('relation_violation', False)),\n            ontology_override_signal = bool(meta.get('ontology_support', False)),\n            config        = GUARD_CFG,\n        )\n        guarded_pairs.append({**p, 'guards': guards})\n\n    return {\n        'question':   question,\n        'provider':   provider,\n        'model':      model_id,\n        'steps':      steps,\n        'concepts':   [[{k: v for k, v in c.items() if k != 'scores'} | {'confidence': (c.get('scores') or {}).get('confidence', 0.0)}\n                        for c in step_cands] for step_cands in concepts],\n        'pairs':      guarded_pairs,\n        'duration_s': round(time.time() - t0, 2),\n        'errors':     cot.get('errors', []),\n    }\n\nprint('Pipeline runner defined.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Run Pipeline Across Models via OpenRouter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Using 2 models for fast iteration. Add more for camera-ready.\n\nOPENROUTER_MODELS = {\n    'claude-haiku':   'anthropic/claude-haiku-4-5',\n    'gpt-4o-mini':    'openai/gpt-4o-mini',\n    # Uncomment for full benchmark:\n    # 'gemini-flash':   'google/gemini-flash-1.5',\n    # 'llama-3-70b':    'meta-llama/llama-3.3-70b-instruct',\n}\n\nSLEEP_BETWEEN_CALLS = 0.2\nN_QUESTIONS = len(QUESTIONS)\n\n# FORCE_RERUN = True to regenerate after pipeline fixes; set False once results look good.\nFORCE_RERUN = True\n\ndef _cache_has_umls(results):\n    \"\"\"Check if cached results contain real UMLS concept data.\"\"\"\n    for r in results:\n        for step_concepts in r.get('concepts', []):\n            for c in step_concepts:\n                if c.get('cui') or c.get('valid'):\n                    return True\n    return False\n\nall_results = {}\n\nfor label, model_slug in OPENROUTER_MODELS.items():\n    cache_path = RESULTS_DIR / f'exp1_{label}_results.json'\n\n    if cache_path.exists() and not FORCE_RERUN:\n        with open(cache_path) as f:\n            cached = json.load(f)\n\n        if umls_configured() and not _cache_has_umls(cached):\n            print(f'[{label}] Cache has NO UMLS data â€” deleting stale cache')\n            cache_path.unlink()\n        else:\n            print(f'[{label}] Loading cached results from {cache_path}')\n            all_results[label] = cached\n            print(f'  Loaded {len(cached)} results')\n            continue\n    elif cache_path.exists() and FORCE_RERUN:\n        print(f'[{label}] FORCE_RERUN=True â€” deleting stale cache')\n        cache_path.unlink()\n\n    print(f'\\n[{label}] ({model_slug}) â€” running {N_QUESTIONS} questions...')\n    results = []\n\n    for i, q in enumerate(QUESTIONS[:N_QUESTIONS]):\n        try:\n            r = run_full_pipeline(q, prefer='openrouter', model=model_slug)\n            results.append(r)\n            label_counts = {lbl: sum(1 for p in r['pairs'] if p['final_label'] == lbl)\n                            for lbl in ['entailment', 'neutral', 'contradiction']}\n            print(f'  [{i+1}/{N_QUESTIONS}] {q[:50]}...'\n                  f'  steps={len(r[\"steps\"])} {label_counts}')\n        except Exception as e:\n            print(f'  [{i+1}] ERROR: {e}')\n            results.append({'question': q, 'provider': 'openrouter', 'model': model_slug,\n                            'steps': [], 'concepts': [], 'pairs': [],\n                            'duration_s': 0, 'errors': [str(e)]})\n        time.sleep(SLEEP_BETWEEN_CALLS)\n\n    with open(cache_path, 'w') as f:\n        json.dump(results, f, indent=2, ensure_ascii=False)\n    print(f'  Saved to {cache_path}')\n    all_results[label] = results\n\nprint(f'\\nAll models done. ({len(all_results)} models Ã— {N_QUESTIONS} questions)')\nprint(f'UMLS configured: {umls_configured()}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Build Analysis DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rows = []\n",
    "pair_rows = []  # one row per step-pair (for depth analysis)\n",
    "\n",
    "for prefer, results in all_results.items():\n",
    "    for r in results:\n",
    "        pairs   = r.get('pairs', [])\n",
    "        steps   = r.get('steps', [])\n",
    "        concepts = r.get('concepts', [])\n",
    "        \n",
    "        if not steps:\n",
    "            continue\n",
    "        \n",
    "        n_pairs  = len(pairs)\n",
    "        n_contra = sum(1 for p in pairs if p.get('final_label') == 'contradiction')\n",
    "        n_entail = sum(1 for p in pairs if p.get('final_label') == 'entailment')\n",
    "        n_neutral = sum(1 for p in pairs if p.get('final_label') == 'neutral')\n",
    "        \n",
    "        # Concept validity\n",
    "        all_cands = [c for step_cands in concepts for c in step_cands]\n",
    "        total_cands  = len(all_cands)\n",
    "        valid_cands  = sum(1 for c in all_cands if c.get('valid'))\n",
    "        steps_with_valid = sum(1 for sc in concepts if any(c.get('valid') for c in sc))\n",
    "        \n",
    "        # Guard signal counts\n",
    "        all_guards = [g for p in pairs for g in p.get('guards', [])]\n",
    "        \n",
    "        # Avg NLI probs\n",
    "        avg_p_contra = np.mean([p.get('probs', {}).get('contradiction', 0) for p in pairs]) if pairs else np.nan\n",
    "        avg_p_entail = np.mean([p.get('probs', {}).get('entailment', 0) for p in pairs]) if pairs else np.nan\n",
    "        \n",
    "        rows.append({\n",
    "            'model_prefer': prefer,\n",
    "            'model_actual': r.get('model', prefer),\n",
    "            'question': r['question'][:70],\n",
    "            'n_steps': len(steps),\n",
    "            'n_pairs': n_pairs,\n",
    "            'n_contradiction': n_contra,\n",
    "            'n_entailment': n_entail,\n",
    "            'n_neutral': n_neutral,\n",
    "            'contradiction_rate': n_contra / n_pairs if n_pairs else np.nan,\n",
    "            'entailment_rate': n_entail / n_pairs if n_pairs else np.nan,\n",
    "            'concepts_total': total_cands,\n",
    "            'concepts_valid': valid_cands,\n",
    "            'concept_valid_rate': valid_cands / total_cands if total_cands else np.nan,\n",
    "            'steps_with_valid_concept': steps_with_valid,\n",
    "            'step_coverage_rate': steps_with_valid / len(steps) if steps else np.nan,\n",
    "            'n_guards_total': len(all_guards),\n",
    "            'n_caution_band': all_guards.count('caution_band'),\n",
    "            'n_lexical_dup': all_guards.count('lexical_duplicate'),\n",
    "            'n_direction_conflict': all_guards.count('direction_conflict'),\n",
    "            'avg_prob_contradiction': avg_p_contra,\n",
    "            'avg_prob_entailment': avg_p_entail,\n",
    "            'duration_s': r.get('duration_s', 0),\n",
    "            'has_error': bool(r.get('errors')),\n",
    "        })\n",
    "        \n",
    "        # Per-pair rows for depth analysis\n",
    "        for p in pairs:\n",
    "            depth = p.get('step_pair', [0, 1])[0]  # i index = depth\n",
    "            pair_rows.append({\n",
    "                'model_prefer': prefer,\n",
    "                'question': r['question'][:50],\n",
    "                'depth': depth,\n",
    "                'label': p.get('final_label', 'unknown'),\n",
    "                'prob_contradiction': p.get('probs', {}).get('contradiction', 0),\n",
    "                'prob_entailment': p.get('probs', {}).get('entailment', 0),\n",
    "                'prob_neutral': p.get('probs', {}).get('neutral', 0),\n",
    "                'guards': '|'.join(p.get('guards', [])),\n",
    "                'umls_jaccard': p.get('meta', {}).get('umls_overlap_jaccard', 0),\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df_pairs = pd.DataFrame(pair_rows)\n",
    "\n",
    "print(f'Summary DataFrame: {len(df)} questions x {len(df.columns)} columns')\n",
    "print(f'Pairs DataFrame:   {len(df_pairs)} pairs x {len(df_pairs.columns)} columns')\n",
    "\n",
    "# Save\n",
    "df.to_csv(RESULTS_DIR / 'exp1_summary.csv', index=False)\n",
    "df_pairs.to_csv(RESULTS_DIR / 'exp1_pairs.csv', index=False)\n",
    "print('Saved CSVs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Table 1: Per-Model Summary Statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "summary = df.groupby('model_prefer').agg(\n",
    "    n_questions       = ('question', 'count'),\n",
    "    avg_steps         = ('n_steps', 'mean'),\n",
    "    avg_pairs         = ('n_pairs', 'mean'),\n",
    "    contradiction_rate = ('contradiction_rate', 'mean'),\n",
    "    entailment_rate   = ('entailment_rate', 'mean'),\n",
    "    concept_valid_rate = ('concept_valid_rate', 'mean'),\n",
    "    step_coverage_rate = ('step_coverage_rate', 'mean'),\n",
    "    avg_prob_contra   = ('avg_prob_contradiction', 'mean'),\n",
    "    avg_prob_entail   = ('avg_prob_entailment', 'mean'),\n",
    "    caution_band_rate = ('n_caution_band', 'mean'),\n",
    ").round(4)\n",
    "\n",
    "print('=== Table 1: Per-Model Summary ===')\n",
    "print(summary.T.to_string())\n",
    "summary.to_csv(RESULTS_DIR / 'exp1_model_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Figure 1: Contradiction Rate per Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "models = df['model_prefer'].unique()\n",
    "colors = ['#4C72B0', '#DD8452', '#55A868', '#C44E52']\n",
    "color_map = dict(zip(models, colors))\n",
    "\n",
    "# (a) Contradiction rate distribution per model\n",
    "ax = axes[0]\n",
    "for model in models:\n",
    "    sub = df[df['model_prefer'] == model]['contradiction_rate'].dropna()\n",
    "    ax.boxplot(sub, positions=[list(models).index(model)], widths=0.5,\n",
    "               patch_artist=True,\n",
    "               boxprops=dict(facecolor=color_map[model], alpha=0.7))\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylabel('Contradiction Rate per Question')\n",
    "ax.set_title('(a) Contradiction Rate Distribution')\n",
    "ax.axhline(0, color='grey', lw=0.5, linestyle='--')\n",
    "\n",
    "# (b) Label breakdown stacked bar\n",
    "ax = axes[1]\n",
    "bar_data = df.groupby('model_prefer')[['n_contradiction', 'n_neutral', 'n_entailment']].mean()\n",
    "bar_data.plot(kind='bar', stacked=True, ax=ax,\n",
    "              color=['#C44E52', '#8172B2', '#4C72B0'], alpha=0.85)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Avg. # Pairs per Question')\n",
    "ax.set_title('(b) Label Breakdown (avg per question)')\n",
    "ax.legend(['Contradiction', 'Neutral', 'Entailment'], loc='upper right')\n",
    "ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# (c) Concept valid rate vs contradiction rate scatter\n",
    "ax = axes[2]\n",
    "clean = df.dropna(subset=['concept_valid_rate', 'contradiction_rate'])\n",
    "if clean.empty or clean['concept_valid_rate'].isna().all():\n",
    "    ax.text(0.5, 0.5,\n",
    "            'UMLS not configured\\nSet UMLS_API_KEY to enable\\nconcept validity scoring',\n",
    "            ha='center', va='center', transform=ax.transAxes,\n",
    "            fontsize=10, color='grey', style='italic',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='#f0f0f0', alpha=0.8))\n",
    "    ax.set_title('(c) Concept Validity vs. Contradiction Rate\\n(UMLS required)')\n",
    "else:\n",
    "    for model in models:\n",
    "        sub = clean[clean['model_prefer'] == model]\n",
    "        ax.scatter(sub['concept_valid_rate'], sub['contradiction_rate'],\n",
    "                   label=model, alpha=0.6, s=40, color=color_map[model])\n",
    "    ax.set_xlabel('Concept Validity Rate (UMLS)')\n",
    "    ax.set_ylabel('Contradiction Rate')\n",
    "    ax.set_title('(c) Concept Validity vs. Contradiction Rate')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Experiment 1: Cross-Model Semantic Leakage Benchmark', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp1_fig1_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 1 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Figure 2: Contradiction Rate by Reasoning Depth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# (a) Contradiction rate by depth per model\n",
    "ax = axes[0]\n",
    "max_depth = df_pairs['depth'].max()\n",
    "\n",
    "for model in models:\n",
    "    sub = df_pairs[df_pairs['model_prefer'] == model]\n",
    "    depth_rates = (sub.groupby('depth')['label']\n",
    "                   .apply(lambda x: (x == 'contradiction').mean())\n",
    "                   .reset_index(name='contra_rate'))\n",
    "    ax.plot(depth_rates['depth'], depth_rates['contra_rate'],\n",
    "            marker='o', label=model, color=color_map.get(model), linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Step Pair Depth (i â†’ i+1 position)')\n",
    "ax.set_ylabel('Contradiction Rate')\n",
    "ax.set_title('(a) Contradiction Rate by Reasoning Depth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Avg P(contradiction) by depth â€” all models combined\n",
    "ax = axes[1]\n",
    "for model in models:\n",
    "    sub = df_pairs[df_pairs['model_prefer'] == model]\n",
    "    depth_probs = sub.groupby('depth')['prob_contradiction'].mean().reset_index()\n",
    "    ax.plot(depth_probs['depth'], depth_probs['prob_contradiction'],\n",
    "            marker='s', label=model, color=color_map.get(model), linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Step Pair Depth')\n",
    "ax.set_ylabel('Avg P(contradiction)')\n",
    "ax.set_title('(b) Average P(contradiction) by Depth')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Semantic Leakage Grows with Reasoning Depth', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp1_fig2_leakage_by_depth.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 2 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Figure 3: Guard Signal Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "\n",
    "GUARD_DISPLAY = {\n",
    "    'n_caution_band':      ('caution_band',      '#4C72B0'),\n",
    "    'n_lexical_dup':       ('lexical_duplicate',  '#DD8452'),\n",
    "    'n_direction_conflict': ('direction_conflict', '#55A868'),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# â”€â”€ (a) Guard frequency per model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0]\n",
    "x      = np.arange(len(models))\n",
    "width  = 0.25\n",
    "offset = -(len(GUARD_DISPLAY) - 1) * width / 2\n",
    "\n",
    "for k, (col, (label, color)) in enumerate(GUARD_DISPLAY.items()):\n",
    "    means = [df[df['model_prefer'] == m][col].mean() for m in models]\n",
    "    bars  = ax.bar(x + offset + k * width, means, width, label=label,\n",
    "                   color=color, alpha=0.85)\n",
    "    # annotate \"0\" on genuinely zero bars so they're not invisible\n",
    "    for bar, val in zip(bars, means):\n",
    "        if val < 0.05:\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, 0.05,\n",
    "                    '0', ha='center', va='bottom', fontsize=7, color='grey')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Avg. # Guards per Question')\n",
    "ax.set_title('(a) Guard Signal Frequency per Model')\n",
    "ax.legend()\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "# Note if direction_conflict is all zero (requires bidirectional NLI + data)\n",
    "if df['n_direction_conflict'].max() == 0:\n",
    "    ax.annotate('direction_conflict requires\\nbidirectional NLI scoring',\n",
    "                xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "                ha='center', va='top', fontsize=7,\n",
    "                color='grey', style='italic')\n",
    "\n",
    "# â”€â”€ (b) Guard rate: contradiction pairs vs all other pairs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1]\n",
    "\n",
    "is_contra    = df_pairs['label'] == 'contradiction'\n",
    "n_contra     = is_contra.sum()\n",
    "n_other      = (~is_contra).sum()\n",
    "\n",
    "guard_names  = ['caution_band', 'direction_conflict']\n",
    "guard_colors = ['#4C72B0', '#55A868']\n",
    "bar_width    = 0.35\n",
    "\n",
    "for gi, (guard, color) in enumerate(zip(guard_names, guard_colors)):\n",
    "    has_guard = df_pairs['guards'].str.contains(guard, na=False)\n",
    "\n",
    "    rate_contra = has_guard[is_contra].mean()  if n_contra > 0 else 0.0\n",
    "    rate_other  = has_guard[~is_contra].mean() if n_other  > 0 else 0.0\n",
    "\n",
    "    xs = np.array([gi * 2, gi * 2 + 1], dtype=float)\n",
    "    ax.bar(xs[0], rate_contra, bar_width, color=color, alpha=0.85,\n",
    "           label=f'{guard} (n={int(has_guard[is_contra].sum())})')\n",
    "    ax.bar(xs[1], rate_other,  bar_width, color=color, alpha=0.45)\n",
    "\n",
    "    for x_pos, rate in [(xs[0], rate_contra), (xs[1], rate_other)]:\n",
    "        if rate < 0.01:\n",
    "            ax.text(x_pos, 0.005, '0', ha='center', va='bottom',\n",
    "                    fontsize=8, color='grey')\n",
    "\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(['caution_band\\n(contra)', 'caution_band\\n(other)',\n",
    "                     'dir_conflict\\n(contra)', 'dir_conflict\\n(other)'])\n",
    "ax.set_ylabel('Fraction of Pairs with Guard')\n",
    "ax.set_title(f'(b) Guard Rate: Contradiction vs. Other Pairs\\n'\n",
    "             f'(contradiction pairs n={n_contra}, other n={n_other})')\n",
    "ax.legend(fontsize=8)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "if n_contra == 0:\n",
    "    ax.text(0.5, 0.5,\n",
    "            'No contradiction pairs\\nin this dataset',\n",
    "            ha='center', va='center', transform=ax.transAxes,\n",
    "            fontsize=10, color='grey', style='italic')\n",
    "\n",
    "plt.suptitle('Guard Signal Analysis', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp1_fig3_guard_signals.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print diagnostic\n",
    "print(\"Guard firing rates:\")\n",
    "for guard in ['caution_band', 'lexical_duplicate', 'direction_conflict']:\n",
    "    has = df_pairs['guards'].str.contains(guard, na=False)\n",
    "    print(f\"  {guard:22s}: {has.mean():.3f} overall  \"\n",
    "          f\"(contra={has[is_contra].mean():.3f}  other={has[~is_contra].mean():.3f})\")\n",
    "print(f'Figure 3 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012",
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Statistical Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nfrom scipy.stats import mannwhitneyu, kruskal, spearmanr\nimport numpy as np\n\nprint('=== Statistical Tests ===\\n')\n\n# Kruskal-Wallis: do models differ in contradiction rate?\ngroups = [df[df['model_prefer'] == m]['contradiction_rate'].dropna().values\n          for m in models]\nif all(len(g) > 1 for g in groups) and len(groups) > 1:\n    all_vals = np.concatenate(groups)\n    if np.ptp(all_vals) > 0:\n        stat, p = kruskal(*groups)\n        print(f'Kruskal-Wallis test (contradiction rate across models):')\n        print(f'  H={stat:.3f}, p={p:.4f}')\n        print(f'  Interpretation: {\"Models differ significantly\" if p < 0.05 else \"No significant difference\"} (Î±=0.05)')\n    else:\n        print(f'Kruskal-Wallis test: SKIPPED â€” all contradiction rates identical ({all_vals[0]:.4f})')\n\n# Spearman correlation: concept validity vs contradiction rate\nclean = df.dropna(subset=['concept_valid_rate', 'contradiction_rate'])\nif len(clean) > 5 and clean['concept_valid_rate'].std() > 0 and clean['contradiction_rate'].std() > 0:\n    rho, p_rho = spearmanr(clean['concept_valid_rate'], clean['contradiction_rate'])\n    print(f'\\nSpearman Ï (concept validity vs contradiction rate):')\n    print(f'  Ï={rho:.3f}, p={p_rho:.4f}')\n    print(f'  Interpretation: {\"Significant negative correlation\" if rho < 0 and p_rho < 0.05 else \"No significant correlation\"}')\nelif len(clean) > 5:\n    print(f'\\nSpearman Ï: SKIPPED â€” no variance in one or both variables')\n\n# Trend test: does contradiction rate increase with depth?\nall_depth = df_pairs.groupby('depth')['label'].apply(lambda x: (x == 'contradiction').mean())\nif len(all_depth) > 2 and all_depth.std() > 0:\n    rho_d, p_d = spearmanr(all_depth.index, all_depth.values)\n    print(f'\\nSpearman Ï (depth vs contradiction rate):')\n    print(f'  Ï={rho_d:.3f}, p={p_d:.4f}')\n    print(f'  Interpretation: {\"Contradiction increases with depth\" if rho_d > 0 and p_d < 0.05 else \"No significant trend\"}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Top Contradiction Examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print('=== Top Contradiction Examples ===\\n')\n",
    "\n",
    "top_contra = df_pairs[df_pairs['label'] == 'contradiction'].nlargest(5, 'prob_contradiction')\n",
    "\n",
    "for idx, row in top_contra.iterrows():\n",
    "    print(f\"Model: {row['model_prefer']} | Depth: {row['depth']} | P(contra): {row['prob_contradiction']:.3f}\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Guards: {row['guards'] or 'none'}\")\n",
    "    print('-' * 70)\n",
    "\n",
    "# Save enriched pairs for other notebooks\n",
    "df_pairs.to_json(RESULTS_DIR / 'exp1_pairs_enriched.json', orient='records', indent=2)\n",
    "df.to_json(RESULTS_DIR / 'exp1_summary_enriched.json', orient='records', indent=2)\n",
    "print('\\nEnriched results saved for Experiments 2, 3, 4.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Key findings from this experiment:\n",
    "\n",
    "1. **Contradiction rate** varies across LLMs â€” check Table 1 above\n",
    "2. **Depth effect** â€” contradiction rate trends upward at later reasoning steps\n",
    "3. **Concept validity** â€” higher UMLS concept validity correlates with lower contradiction rate\n",
    "4. **Guard signals** â€” `caution_band` and `direction_conflict` fire more often in contradiction pairs\n",
    "\n",
    "These results go into **Section 4 (Results)** of the paper:\n",
    "- Table 1 â†’ summary statistics\n",
    "- Figure 1 â†’ model comparison\n",
    "- Figure 2 â†’ depth analysis\n",
    "- Figure 3 â†’ guard signals\n",
    "- Statistical tests â†’ significance\n"
   ]
  }
 ]
}