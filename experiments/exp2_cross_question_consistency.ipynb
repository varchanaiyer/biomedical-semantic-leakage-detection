{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b001",
   "metadata": {},
   "source": [
    "# Experiment 2: Cross-Question Ontological Consistency\n",
    "\n",
    "**Research Question:** Do LLMs contradict themselves *across different questions* about the same medical concept?\n",
    "\n",
    "**Core idea:** The standard pipeline checks if Step 2 contradicts Step 3 *within one answer*. This experiment checks if the LLM says something different about the *same concept* (e.g., aspirin) across *different questions*.\n",
    "\n",
    "**Method:**\n",
    "1. Group questions by shared medical concept (drug / disease / mechanism)\n",
    "2. Run the pipeline on each group \u2014 collect all CoT steps mentioning that concept\n",
    "3. Run NLI across steps from *different questions* that share the concept\n",
    "4. Compute a **cross-response contradiction score** per concept\n",
    "\n",
    "**Why it matters:** An LLM might be internally consistent within each answer but systematically contradict itself across responses \u2014 a critical reliability problem for clinical decision support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-colab-exp2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# SETUP: Clone repo, install deps, set API keys\n# Run this cell first \u2014 works in Colab and local Jupyter\n# ============================================================\nimport os, sys\nfrom pathlib import Path\n\n# \u2500\u2500 1. Clone or update the repository \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nREPO_URL  = 'https://github.com/varchanaiyer/biomedical-semantic-leakage-detection'\nREPO_DIR  = 'biomedical-semantic-leakage-detection'\n\nif not Path(REPO_DIR).exists():\n    os.system(f'git clone {REPO_URL}')\nelse:\n    os.system(f'git -C {REPO_DIR} pull --quiet')\n\n# \u2500\u2500 2. Add project root to path \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n_cwd = Path(os.getcwd())\nif (_cwd / REPO_DIR / 'utils').exists():\n    PROJECT_ROOT = str(_cwd / REPO_DIR)\nelif (_cwd / 'utils').exists():\n    PROJECT_ROOT = str(_cwd)\nelif (_cwd.parent / 'utils').exists():\n    PROJECT_ROOT = str(_cwd.parent)\nelse:\n    PROJECT_ROOT = str(_cwd / REPO_DIR)  # fallback\n\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\nos.chdir(PROJECT_ROOT)\nprint(f'PROJECT_ROOT: {PROJECT_ROOT}')\n\n# \u2500\u2500 3. Install dependencies \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nos.system('pip install openai numpy pandas scipy scikit-learn matplotlib seaborn requests jupyter --quiet') ipywidgets\n\n\nprint('Setup complete. API keys configured:', {\n    k: ('set' if os.environ.get(k) else 'NOT SET')\n    for k in ['OPENROUTER_API_KEY','ANTHROPIC_API_KEY','OPENAI_API_KEY','UMLS_API_KEY']\n})\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "setup-colab-exp2-keys",
   "metadata": {},
   "source": [
    "# \u2500\u2500 OpenRouter API Key \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os, importlib.util\nfrom IPython.display import display, clear_output, HTML\n\n_HAS_WIDGETS = importlib.util.find_spec(\"ipywidgets\") is not None\n\nif _HAS_WIDGETS:\n    import ipywidgets as widgets\n\n    _key_box = widgets.Password(\n        placeholder=\"sk-or-v1-\u2026  (get yours free at openrouter.ai)\",\n        layout=widgets.Layout(width=\"520px\"),\n    )\n    _btn = widgets.Button(\n        description=\"Set Key\", button_style=\"primary\",\n        icon=\"check\", layout=widgets.Layout(width=\"110px\"),\n    )\n    _out = widgets.Output()\n\n    def _apply(_b):\n        with _out:\n            clear_output()\n            key = _key_box.value.strip()\n            if key:\n                os.environ[\"OPENROUTER_API_KEY\"] = key\n                print(f\"  \u2713 OpenRouter key set ({len(key)} chars)\")\n            else:\n                print(\"  \u2717 Paste your OpenRouter key above, then click Set Key.\")\n\n    _btn.on_click(_apply)\n    display(HTML(\"<b>\ud83d\udd11 OpenRouter API Key</b>\"))\n    display(widgets.HBox([_key_box, _btn]))\n    display(_out)\n    display(HTML(\n        \"<small>Get a free key at \"\n        \"<a href=\\\"https://openrouter.ai\\\" target=\\\"_blank\\\">openrouter.ai</a>\"\n        \" \u2014 the notebooks will automatically run across all configured models.</small>\"\n    ))\nelse:\n    os.environ.setdefault(\"OPENROUTER_API_KEY\", \"\")\n    print(\"ipywidgets not found \u2014 set key with:\")\n    print(\"  os.environ[\\\"OPENROUTER_API_KEY\\\"] = \\\"sk-or-v1-...\\\"\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, time, itertools\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Project root (setup cell already set CWD and sys.path; this is a fallback for local use)\n",
    "_cwd = Path(os.getcwd())\n",
    "PROJECT_ROOT = _cwd if (_cwd / 'utils').exists() else _cwd.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "RESULTS_DIR = PROJECT_ROOT / 'experiments' / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.cot_generator import generate as generate_cot\n",
    "from utils.concept_extractor import extract_concepts\n",
    "from utils.hybrid_checker import build_entailment_records\n",
    "from utils.entailment_checker import check_entailment\n",
    "from utils.umls_api_linker import is_configured as umls_configured\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Modules loaded.')\n",
    "print(f'UMLS configured: {umls_configured()}')\n",
    "\n",
    "SLEEP = 0.5  # seconds between API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Concept-Grouped Question Bank \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Questions are grouped by a key concept. We'll collect CoT steps\n",
    "# for each group and look for cross-answer contradictions.\n",
    "\n",
    "CONCEPT_GROUPS = {\n",
    "    'aspirin': [\n",
    "        \"Does aspirin reduce the risk of myocardial infarction?\",\n",
    "        \"How does aspirin reduce platelet aggregation?\",\n",
    "        \"What are the risks of long-term aspirin therapy?\",\n",
    "        \"Does aspirin increase the risk of gastrointestinal bleeding?\",\n",
    "        \"Is aspirin effective for primary prevention of cardiovascular disease?\",\n",
    "        \"How does aspirin affect prostaglandin synthesis?\",\n",
    "    ],\n",
    "    'metformin': [\n",
    "        \"How does metformin lower blood glucose in type 2 diabetes?\",\n",
    "        \"What are the contraindications of metformin?\",\n",
    "        \"Does metformin cause lactic acidosis?\",\n",
    "        \"What is the effect of metformin on cardiovascular outcomes?\",\n",
    "        \"How does metformin affect hepatic glucose production?\",\n",
    "    ],\n",
    "    'statins': [\n",
    "        \"How do statins reduce LDL cholesterol?\",\n",
    "        \"Do statins reduce the risk of stroke?\",\n",
    "        \"What is the mechanism of statin-induced myopathy?\",\n",
    "        \"Do statins reduce mortality in patients with heart failure?\",\n",
    "        \"What is the role of statins in primary prevention of cardiovascular disease?\",\n",
    "    ],\n",
    "    'insulin': [\n",
    "        \"How does insulin regulate blood glucose levels?\",\n",
    "        \"What is the mechanism of insulin resistance in type 2 diabetes?\",\n",
    "        \"What are the risks of insulin therapy in type 1 diabetes?\",\n",
    "        \"How does insulin affect lipid metabolism?\",\n",
    "        \"What is the difference between basal and bolus insulin?\",\n",
    "    ],\n",
    "    'ace_inhibitors': [\n",
    "        \"How do ACE inhibitors reduce blood pressure?\",\n",
    "        \"What is the role of ACE inhibitors in heart failure?\",\n",
    "        \"Do ACE inhibitors protect renal function in diabetic nephropathy?\",\n",
    "        \"What are the adverse effects of ACE inhibitors?\",\n",
    "        \"Can ACE inhibitors cause hyperkalemia?\",\n",
    "    ],\n",
    "    'beta_blockers': [\n",
    "        \"How do beta-blockers reduce heart rate?\",\n",
    "        \"Do beta-blockers improve survival after myocardial infarction?\",\n",
    "        \"What are the contraindications of beta-blockers?\",\n",
    "        \"How do beta-blockers affect cardiac output?\",\n",
    "        \"What is the role of beta-blockers in treating hypertension?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "total_questions = sum(len(qs) for qs in CONCEPT_GROUPS.values())\n",
    "print(f'Concept groups: {len(CONCEPT_GROUPS)}')\n",
    "print(f'Total questions: {total_questions}')\n",
    "for concept, qs in CONCEPT_GROUPS.items():\n",
    "    print(f'  {concept}: {len(qs)} questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Run Pipeline on All Concept-Grouped Questions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# OpenRouter model to use for all exp2 queries\n",
    "# Change this slug to try a different model (see https://openrouter.ai/models)\n",
    "PREFER = 'openrouter'\n",
    "OPENROUTER_MODEL = 'anthropic/claude-haiku-4-5'  # swap to 'openai/gpt-4o-mini', 'google/gemini-flash-1.5', etc.  # uses OpenRouter API key from config.py\n",
    "CACHE_FILE = RESULTS_DIR / f'exp2_{PREFER}_concept_results.json'\n",
    "\n",
    "def run_concept_pipeline(question: str, prefer: str = 'openrouter', model: str = None) -> dict:\n",
    "    cot = generate_cot(question, prefer=prefer, model=model)\n",
    "    steps = cot.get('steps', [])\n",
    "    concepts = extract_concepts(steps, scispacy_when='never', top_k=3)\n",
    "    return {\n",
    "        'question': question,\n",
    "        'model': cot.get('model', 'unknown'),\n",
    "        'steps': steps,\n",
    "        'concepts': concepts,\n",
    "        'errors': cot.get('errors', []),\n",
    "    }\n",
    "\n",
    "if CACHE_FILE.exists():\n",
    "    print(f'Loading cached results from {CACHE_FILE}')\n",
    "    with open(CACHE_FILE) as f:\n",
    "        group_results = json.load(f)\n",
    "else:\n",
    "    group_results = {}  # {concept: [result, ...]}\n",
    "    for concept, questions in CONCEPT_GROUPS.items():\n",
    "        print(f'\\nRunning concept group: {concept} ({len(questions)} questions)')\n",
    "        results = []\n",
    "        for q in questions:\n",
    "            try:\n",
    "                r = run_concept_pipeline(q, prefer=PREFER, model=OPENROUTER_MODEL)\n",
    "                results.append(r)\n",
    "                print(f'  OK | steps={len(r[\"steps\"])} | model={r[\"model\"]} | q={q[:55]}...')\n",
    "            except Exception as e:\n",
    "                print(f'  ERROR: {e}')\n",
    "                results.append({'question': q, 'model': 'error', 'steps': [], 'concepts': [], 'errors': [str(e)]})\n",
    "            time.sleep(SLEEP)\n",
    "        group_results[concept] = results\n",
    "    \n",
    "    with open(CACHE_FILE, 'w') as f:\n",
    "        json.dump(group_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f'\\nCached to {CACHE_FILE}')\n",
    "\n",
    "print('\\nDone. Concept groups loaded:')\n",
    "for c, rs in group_results.items():\n",
    "    ok = sum(1 for r in rs if r.get('steps'))\n",
    "    print(f'  {c}: {ok}/{len(rs)} successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Extract Steps Containing Each Concept \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# For each concept group, collect all CoT steps and the question they came from.\n",
    "# We use simple keyword matching + UMLS CUIs to identify steps mentioning the concept.\n",
    "\n",
    "CONCEPT_KEYWORDS = {\n",
    "    'aspirin':       ['aspirin', 'acetylsalicylic', 'cox-1', 'cox1', 'thromboxane', 'platelet'],\n",
    "    'metformin':     ['metformin', 'biguanide', 'hepatic glucose', 'ampk'],\n",
    "    'statins':       ['statin', 'hmg-coa', 'atorvastatin', 'simvastatin', 'lovastatin', 'ldl'],\n",
    "    'insulin':       ['insulin', 'pancreatic', 'beta cell', 'glucose uptake', 'glycogen'],\n",
    "    'ace_inhibitors': ['ace inhibitor', 'angiotensin', 'ace', 'captopril', 'lisinopril', 'enalapril'],\n",
    "    'beta_blockers': ['beta-blocker', 'beta blocker', 'metoprolol', 'atenolol', 'propranolol', 'adrenergic'],\n",
    "}\n",
    "\n",
    "def step_mentions_concept(step: str, keywords: list) -> bool:\n",
    "    step_lower = step.lower()\n",
    "    return any(kw in step_lower for kw in keywords)\n",
    "\n",
    "# Build: {concept: [(q_idx, step_idx, step_text), ...]}\n",
    "concept_steps = defaultdict(list)  # {concept: [(q_idx, step_idx, step_text, question), ...]}\n",
    "\n",
    "for concept, results in group_results.items():\n",
    "    keywords = CONCEPT_KEYWORDS.get(concept, [concept.replace('_', ' ')])\n",
    "    for q_idx, r in enumerate(results):\n",
    "        steps = r.get('steps', [])\n",
    "        for s_idx, step in enumerate(steps):\n",
    "            # Include ALL steps from this concept group (they're all about the concept)\n",
    "            concept_steps[concept].append({\n",
    "                'q_idx': q_idx,\n",
    "                's_idx': s_idx,\n",
    "                'step': step,\n",
    "                'question': r['question'],\n",
    "                'mentions_concept': step_mentions_concept(step, keywords),\n",
    "            })\n",
    "\n",
    "print('Step extraction complete:')\n",
    "for concept, steps in concept_steps.items():\n",
    "    mentions = sum(1 for s in steps if s['mentions_concept'])\n",
    "    print(f'  {concept}: {len(steps)} total steps, {mentions} directly mention concept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Cross-Response NLI \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# For each concept group:\n",
    "#   - Take pairs of steps from DIFFERENT questions\n",
    "#   - Run NLI entailment\n",
    "#   - Record contradiction rate\n",
    "\n",
    "CROSS_NLI_CACHE = RESULTS_DIR / f'exp2_{PREFER}_cross_nli.json'\n",
    "MAX_PAIRS_PER_CONCEPT = 50  # cap to avoid too many API calls\n",
    "\n",
    "if CROSS_NLI_CACHE.exists():\n",
    "    print(f'Loading cached cross-NLI from {CROSS_NLI_CACHE}')\n",
    "    with open(CROSS_NLI_CACHE) as f:\n",
    "        cross_nli_results = json.load(f)\n",
    "else:\n",
    "    cross_nli_results = {}\n",
    "    \n",
    "    for concept, steps_list in concept_steps.items():\n",
    "        print(f'\\nCross-NLI for concept: {concept}')\n",
    "        \n",
    "        # Get steps from different questions\n",
    "        # Group steps by question index\n",
    "        by_question = defaultdict(list)\n",
    "        for s in steps_list:\n",
    "            by_question[s['q_idx']].append(s)\n",
    "        \n",
    "        q_indices = list(by_question.keys())\n",
    "        cross_pairs = []\n",
    "        \n",
    "        # Generate cross-question pairs\n",
    "        for i, j in itertools.combinations(q_indices, 2):\n",
    "            steps_i = by_question[i]\n",
    "            steps_j = by_question[j]\n",
    "            # Sample a step from each question\n",
    "            for si in steps_i[:2]:  # max 2 steps per question pair\n",
    "                for sj in steps_j[:2]:\n",
    "                    cross_pairs.append((si, sj))\n",
    "                    if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n",
    "                        break\n",
    "                if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n",
    "                    break\n",
    "            if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n",
    "                break\n",
    "        \n",
    "        print(f'  {len(cross_pairs)} cross-question step pairs to evaluate')\n",
    "        \n",
    "        # Run NLI on all cross pairs\n",
    "        pair_texts = [p[0]['step'] for p in cross_pairs]\n",
    "        hyp_texts  = [p[1]['step'] for p in cross_pairs]\n",
    "        \n",
    "        # Use check_entailment with dummy steps (it runs pairwise NLI on adjacent pairs)\n",
    "        # We interleave premise/hypothesis for pairwise NLI\n",
    "        concept_results = []\n",
    "        for (si, sj) in cross_pairs:\n",
    "            try:\n",
    "                nli_out = check_entailment([si['step'], sj['step']])\n",
    "                if nli_out:\n",
    "                    record = nli_out[0]\n",
    "                    concept_results.append({\n",
    "                        'q_idx_a': si['q_idx'],\n",
    "                        'q_idx_b': sj['q_idx'],\n",
    "                        'question_a': si['question'],\n",
    "                        'question_b': sj['question'],\n",
    "                        'step_a': si['step'],\n",
    "                        'step_b': sj['step'],\n",
    "                        'label': record.get('label', 'unknown'),\n",
    "                        'probs': record.get('probs', {}),\n",
    "                        'same_question': si['q_idx'] == sj['q_idx'],\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        cross_nli_results[concept] = concept_results\n",
    "        ok = sum(1 for r in concept_results if r['label'] != 'unknown')\n",
    "        print(f'  {ok}/{len(concept_results)} pairs scored')\n",
    "    \n",
    "    with open(CROSS_NLI_CACHE, 'w') as f:\n",
    "        json.dump(cross_nli_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f'\\nCached to {CROSS_NLI_CACHE}')\n",
    "\n",
    "print('\\nCross-NLI results loaded for all concepts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Compute Cross-Response Contradiction Score \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "rows = []\n",
    "for concept, results in cross_nli_results.items():\n",
    "    for r in results:\n",
    "        rows.append({\n",
    "            'concept': concept,\n",
    "            'q_idx_a': r.get('q_idx_a'),\n",
    "            'q_idx_b': r.get('q_idx_b'),\n",
    "            'question_a': r.get('question_a', '')[:60],\n",
    "            'question_b': r.get('question_b', '')[:60],\n",
    "            'step_a': r.get('step_a', '')[:80],\n",
    "            'step_b': r.get('step_b', '')[:80],\n",
    "            'label': r.get('label', 'unknown'),\n",
    "            'prob_contradiction': (r.get('probs') or {}).get('contradiction', 0),\n",
    "            'prob_entailment': (r.get('probs') or {}).get('entailment', 0),\n",
    "            'prob_neutral': (r.get('probs') or {}).get('neutral', 0),\n",
    "            'same_question': r.get('same_question', False),\n",
    "        })\n",
    "\n",
    "df_cross = pd.DataFrame(rows)\n",
    "\n",
    "# Per-concept cross-response contradiction rate\n",
    "concept_stats = df_cross[~df_cross['same_question']].groupby('concept').agg(\n",
    "    n_pairs          = ('label', 'count'),\n",
    "    contradiction_rate = ('label', lambda x: (x == 'contradiction').mean()),\n",
    "    entailment_rate  = ('label', lambda x: (x == 'entailment').mean()),\n",
    "    avg_p_contra     = ('prob_contradiction', 'mean'),\n",
    "    avg_p_entail     = ('prob_entailment', 'mean'),\n",
    ").round(4).sort_values('contradiction_rate', ascending=False)\n",
    "\n",
    "print('=== Cross-Response Contradiction Rates by Concept ===\\n')\n",
    "print(concept_stats.to_string())\n",
    "concept_stats.to_csv(RESULTS_DIR / 'exp2_concept_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Figure 1: Cross-Response vs. Within-Response Contradiction Rate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Also compute within-response contradiction rates for comparison\n",
    "within_rows = []\n",
    "for concept, results in group_results.items():\n",
    "    for r in results:\n",
    "        steps = r.get('steps', [])\n",
    "        concepts = r.get('concepts', [])\n",
    "        if len(steps) < 2:\n",
    "            continue\n",
    "        pairs = build_entailment_records(steps, concepts)\n",
    "        for p in pairs:\n",
    "            within_rows.append({\n",
    "                'concept': concept,\n",
    "                'label': p.get('final_label', 'unknown'),\n",
    "                'prob_contradiction': p.get('probs', {}).get('contradiction', 0),\n",
    "            })\n",
    "\n",
    "df_within = pd.DataFrame(within_rows)\n",
    "within_stats = df_within.groupby('concept').agg(\n",
    "    contradiction_rate = ('label', lambda x: (x == 'contradiction').mean())\n",
    ").round(4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# (a) Cross-response contradiction rate per concept\n",
    "ax = axes[0]\n",
    "cross_sorted = concept_stats['contradiction_rate'].sort_values(ascending=False)\n",
    "bars = ax.bar(range(len(cross_sorted)), cross_sorted.values, color='#C44E52', alpha=0.85)\n",
    "ax.set_xticks(range(len(cross_sorted)))\n",
    "ax.set_xticklabels([c.replace('_', ' ') for c in cross_sorted.index], rotation=30, ha='right')\n",
    "ax.set_ylabel('Cross-Response Contradiction Rate')\n",
    "ax.set_title('(a) Cross-Question Contradiction Rate by Concept')\n",
    "\n",
    "# (b) Within vs. cross comparison\n",
    "ax = axes[1]\n",
    "concepts_common = list(set(cross_sorted.index) & set(within_stats.index))\n",
    "x = np.arange(len(concepts_common))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, [within_stats.loc[c, 'contradiction_rate'] for c in concepts_common],\n",
    "       w, label='Within-answer', color='#4C72B0', alpha=0.85)\n",
    "ax.bar(x + w/2, [cross_sorted.get(c, 0) for c in concepts_common],\n",
    "       w, label='Cross-answer', color='#C44E52', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([c.replace('_', ' ') for c in concepts_common], rotation=30, ha='right')\n",
    "ax.set_ylabel('Contradiction Rate')\n",
    "ax.set_title('(b) Within-Answer vs. Cross-Answer Contradictions')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('Experiment 2: Cross-Question Ontological Consistency', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp2_fig1_cross_vs_within.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 1 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Figure 2: Cross-Question Contradiction Heatmap \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# For each concept group, show the pairwise contradiction rate between question pairs\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, (concept, results) in enumerate(group_results.items()):\n",
    "    if ax_idx >= len(axes):\n",
    "        break\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    n_q = len(results)\n",
    "    matrix = np.zeros((n_q, n_q))\n",
    "    count_matrix = np.zeros((n_q, n_q))\n",
    "    \n",
    "    cross_data = df_cross[df_cross['concept'] == concept]\n",
    "    for _, row in cross_data.iterrows():\n",
    "        qi, qj = int(row.get('q_idx_a', 0)), int(row.get('q_idx_b', 0))\n",
    "        if qi < n_q and qj < n_q:\n",
    "            val = 1.0 if row['label'] == 'contradiction' else 0.0\n",
    "            matrix[qi][qj] += val\n",
    "            matrix[qj][qi] += val\n",
    "            count_matrix[qi][qj] += 1\n",
    "            count_matrix[qj][qi] += 1\n",
    "    \n",
    "    # Normalize\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        norm_matrix = np.where(count_matrix > 0, matrix / count_matrix, np.nan)\n",
    "    \n",
    "    im = ax.imshow(norm_matrix, cmap='Reds', vmin=0, vmax=1, aspect='auto')\n",
    "    ax.set_title(f'{concept.replace(\"_\", \" \").title()}')\n",
    "    ax.set_xlabel('Question index')\n",
    "    ax.set_ylabel('Question index')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Cross-Question Contradiction Rate Heatmaps per Concept', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp2_fig2_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 2 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Top Cross-Response Contradiction Examples \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "print('=== Top Cross-Response Contradiction Examples ===\\n')\n",
    "\n",
    "top = df_cross[(df_cross['label'] == 'contradiction') & (~df_cross['same_question'])]\\\n",
    "    .nlargest(8, 'prob_contradiction')\n",
    "\n",
    "for _, row in top.iterrows():\n",
    "    print(f\"Concept: {row['concept']} | P(contra): {row['prob_contradiction']:.3f}\")\n",
    "    print(f\"  Q1: {row['question_a']}\")\n",
    "    print(f\"  Step A: {row['step_a']}\")\n",
    "    print(f\"  Q2: {row['question_b']}\")\n",
    "    print(f\"  Step B: {row['step_b']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Statistical Test: Is Cross-Answer Rate > Within-Answer Rate? \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "\n",
    "within_rates = []\n",
    "cross_rates  = []\n",
    "\n",
    "for concept in CONCEPT_GROUPS.keys():\n",
    "    if concept in within_stats.index:\n",
    "        within_rates.append(float(within_stats.loc[concept, 'contradiction_rate']))\n",
    "    if concept in concept_stats.index:\n",
    "        cross_rates.append(float(concept_stats.loc[concept, 'contradiction_rate']))\n",
    "\n",
    "print('=== Statistical Comparison: Cross-Answer vs. Within-Answer ===\\n')\n",
    "print(f'Within-answer contradiction rates: {[round(x, 3) for x in within_rates]}')\n",
    "print(f'Cross-answer  contradiction rates: {[round(x, 3) for x in cross_rates]}')\n",
    "print(f'Mean within: {np.mean(within_rates):.4f}')\n",
    "print(f'Mean cross:  {np.mean(cross_rates):.4f}')\n",
    "\n",
    "if len(within_rates) >= 3 and len(cross_rates) >= 3:\n",
    "    try:\n",
    "        stat, p = mannwhitneyu(cross_rates, within_rates, alternative='greater')\n",
    "        print(f'\\nMann-Whitney U test (cross > within):')\n",
    "        print(f'  U={stat:.2f}, p={p:.4f}')\n",
    "        if p < 0.05:\n",
    "            print('  \u2713 Cross-answer contradiction rate is SIGNIFICANTLY HIGHER than within-answer.')\n",
    "        else:\n",
    "            print('  \u2717 No significant difference detected (may need more data).')\n",
    "    except Exception as e:\n",
    "        print(f'  Test failed: {e}')\n",
    "\n",
    "print('\\nKey finding: Even when within-answer reasoning is locally consistent,')\n",
    "print('LLMs may assert contradictory claims about the same concept across questions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "1. **Cross-response contradictions are real** \u2014 LLMs assert conflicting claims about the same medical concept across different questions\n",
    "2. **Concept-level variation** \u2014 some concepts (e.g., drug interactions) show higher cross-response contradiction rates than others\n",
    "3. **Heatmap pattern** \u2014 certain question pairs are consistently contradictory, suggesting specific factual inconsistencies\n",
    "\n",
    "**For the paper:**\n",
    "- Table 1 \u2192 concept_stats DataFrame  \n",
    "- Figure 1 \u2192 within vs. cross comparison\n",
    "- Figure 2 \u2192 heatmaps\n",
    "- Section 3 \u2192 describe the cross-response NLI method\n",
    "- Section 4 \u2192 statistical comparison\n",
    "\n",
    "**Implication for workshop:** Addresses the ICLR 2026 topic *\"Avoiding Logical Contradictions Across Responses to Multiple Related Questions\"* directly."
   ]
  }
 ]
}