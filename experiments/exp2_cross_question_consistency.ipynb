{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b001",
   "metadata": {},
   "source": [
    "# Experiment 2: Cross-Question Ontological Consistency\n",
    "\n",
    "**Research Question:** Do LLMs contradict themselves *across different questions* about the same medical concept?\n",
    "\n",
    "**Core idea:** The standard pipeline checks if Step 2 contradicts Step 3 *within one answer*. This experiment checks if the LLM says something different about the *same concept* (e.g., aspirin) across *different questions*.\n",
    "\n",
    "**Method:**\n",
    "1. Group questions by shared medical concept (drug / disease / mechanism)\n",
    "2. Run the pipeline on each group — collect all CoT steps mentioning that concept\n",
    "3. Run NLI across steps from *different questions* that share the concept\n",
    "4. Compute a **cross-response contradiction score** per concept\n",
    "\n",
    "**Why it matters:** An LLM might be internally consistent within each answer but systematically contradict itself across responses — a critical reliability problem for clinical decision support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-colab-exp2",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP: Clone repo, install deps, set API keys\n# Run this cell first — works in Colab and local Jupyter\n# ============================================================\nimport os, sys\nfrom pathlib import Path\n\n# ── 1. Clone or update the repository ───────────────────────\nREPO_URL  = 'https://github.com/varchanaiyer/biomedical-semantic-leakage-detection'\nREPO_DIR  = 'biomedical-semantic-leakage-detection'\n\nif not Path(REPO_DIR).exists():\n    os.system(f'git clone {REPO_URL}')\nelse:\n    os.system(f'git -C {REPO_DIR} pull --quiet')\n\n# ── 2. Add project root to path ─────────────────────────────\n_cwd = Path(os.getcwd())\nif (_cwd / REPO_DIR / 'utils').exists():\n    PROJECT_ROOT = str(_cwd / REPO_DIR)\nelif (_cwd / 'utils').exists():\n    PROJECT_ROOT = str(_cwd)\nelif (_cwd.parent / 'utils').exists():\n    PROJECT_ROOT = str(_cwd.parent)\nelse:\n    PROJECT_ROOT = str(_cwd / REPO_DIR)  # fallback\n\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\nos.chdir(PROJECT_ROOT)\nprint(f'PROJECT_ROOT: {PROJECT_ROOT}')\n\n# ── 3. Install dependencies ──────────────────────────────────\nos.system('pip install openai numpy pandas scipy scikit-learn matplotlib seaborn requests jupyter ipywidgets python-dotenv --quiet')\n\n# ── 4. Set API keys directly ─────────────────────────────────\nos.environ[\"UMLS_API_KEY\"] = \"626c44a6-15bd-4702-9c09-f2f64e483067\"\n\ntry:\n    from dotenv import load_dotenv\n    _env_path = Path(PROJECT_ROOT) / \".env\"\n    if _env_path.exists():\n        load_dotenv(_env_path, override=False)\nexcept ImportError:\n    pass\ntry:\n    import config\n    for _attr, _env in [('ANTHROPIC_API_KEY', 'ANTHROPIC_API_KEY'),\n                         ('OPENAI_API_KEY', 'OPENAI_API_KEY'),\n                         ('OPENROUTER_API_KEY', 'OPENROUTER_API_KEY')]:\n        _val = getattr(config, _attr, '') or ''\n        if _val and not os.environ.get(_env):\n            os.environ[_env] = _val\nexcept ImportError:\n    pass\n\nprint('Setup complete. API keys configured:', {\n    k: ('set' if os.environ.get(k) else 'NOT SET')\n    for k in ['OPENROUTER_API_KEY','ANTHROPIC_API_KEY','OPENAI_API_KEY','UMLS_API_KEY']\n})"
  },
  {
   "cell_type": "code",
   "id": "setup-colab-exp2-keys",
   "metadata": {},
   "source": "# ── API Keys ──────────────────────────────────────────────────────────────────\nimport os, importlib.util\nfrom IPython.display import display, clear_output, HTML\n\n_HAS_WIDGETS = importlib.util.find_spec(\"ipywidgets\") is not None\n\nif _HAS_WIDGETS:\n    import ipywidgets as widgets\n\n    # ── OpenRouter Key ─────────────────────────────────────────\n    _or_box = widgets.Password(\n        placeholder=\"sk-or-v1-…  (get yours free at openrouter.ai)\",\n        layout=widgets.Layout(width=\"520px\"),\n    )\n    _or_btn = widgets.Button(\n        description=\"Set Key\", button_style=\"primary\",\n        icon=\"check\", layout=widgets.Layout(width=\"110px\"),\n    )\n    _or_out = widgets.Output()\n\n    def _apply_or(_b):\n        with _or_out:\n            clear_output()\n            key = _or_box.value.strip()\n            if key:\n                os.environ[\"OPENROUTER_API_KEY\"] = key\n                print(f\"  OpenRouter key set ({len(key)} chars)\")\n            else:\n                print(\"  Paste your OpenRouter key above, then click Set Key.\")\n\n    _or_btn.on_click(_apply_or)\n    display(HTML(\"<b>OpenRouter API Key</b>\"))\n    display(widgets.HBox([_or_box, _or_btn]))\n    display(_or_out)\n    display(HTML(\n        \"<small>Get a free key at \"\n        \"<a href=\\\"https://openrouter.ai\\\" target=\\\"_blank\\\">openrouter.ai</a>\"\n        \"</small>\"\n    ))\n\n    # ── UMLS Key ───────────────────────────────────────────────\n    _umls_box = widgets.Password(\n        placeholder=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx  (get free at uts.nlm.nih.gov)\",\n        layout=widgets.Layout(width=\"520px\"),\n    )\n    _umls_btn = widgets.Button(\n        description=\"Set Key\", button_style=\"primary\",\n        icon=\"check\", layout=widgets.Layout(width=\"110px\"),\n    )\n    _umls_out = widgets.Output()\n\n    def _apply_umls(_b):\n        with _umls_out:\n            clear_output()\n            key = _umls_box.value.strip()\n            if key:\n                os.environ[\"UMLS_API_KEY\"] = key\n                print(f\"  UMLS key set ({len(key)} chars)\")\n            else:\n                print(\"  Paste your UMLS key above, then click Set Key.\")\n\n    _umls_btn.on_click(_apply_umls)\n    display(HTML(\"<br><b>UMLS API Key</b> (required for CUI-based concept matching)\"))\n    display(widgets.HBox([_umls_box, _umls_btn]))\n    display(_umls_out)\n    display(HTML(\n        \"<small>Get a free key at \"\n        \"<a href=\\\"https://uts.nlm.nih.gov/uts/signup-login\\\" target=\\\"_blank\\\">uts.nlm.nih.gov</a>\"\n        \"</small>\"\n    ))\n\n    if os.environ.get(\"UMLS_API_KEY\"):\n        with _umls_out:\n            clear_output()\n            print(f\"  UMLS key already set from environment ({len(os.environ['UMLS_API_KEY'])} chars)\")\n    if os.environ.get(\"OPENROUTER_API_KEY\"):\n        with _or_out:\n            clear_output()\n            print(f\"  OpenRouter key already set from environment ({len(os.environ['OPENROUTER_API_KEY'])} chars)\")\nelse:\n    os.environ.setdefault(\"OPENROUTER_API_KEY\", \"\")\n    os.environ.setdefault(\"UMLS_API_KEY\", \"\")\n    print(\"ipywidgets not found — set keys manually:\")\n    print('  os.environ[\"OPENROUTER_API_KEY\"] = \"sk-or-v1-...\"')\n    print('  os.environ[\"UMLS_API_KEY\"] = \"xxxxxxxx-xxxx-...\"')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, time, itertools\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Project root (setup cell already set CWD and sys.path; this is a fallback for local use)\n",
    "_cwd = Path(os.getcwd())\n",
    "PROJECT_ROOT = _cwd if (_cwd / 'utils').exists() else _cwd.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "RESULTS_DIR = PROJECT_ROOT / 'experiments' / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.cot_generator import generate as generate_cot\n",
    "from utils.concept_extractor import extract_concepts\n",
    "from utils.hybrid_checker import build_entailment_records\n",
    "from utils.entailment_checker import check_entailment\n",
    "from utils.umls_api_linker import is_configured as umls_configured\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Modules loaded.')\n",
    "print(f'UMLS configured: {umls_configured()}')\n",
    "\n",
    "SLEEP = 0.5  # seconds between API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Concept-Grouped Question Bank ─────────────────────────────────────────────\n",
    "# Questions are grouped by a key concept. We'll collect CoT steps\n",
    "# for each group and look for cross-answer contradictions.\n",
    "\n",
    "CONCEPT_GROUPS = {\n",
    "    'aspirin': [\n",
    "        \"Does aspirin reduce the risk of myocardial infarction?\",\n",
    "        \"How does aspirin reduce platelet aggregation?\",\n",
    "        \"What are the risks of long-term aspirin therapy?\",\n",
    "        \"Does aspirin increase the risk of gastrointestinal bleeding?\",\n",
    "        \"Is aspirin effective for primary prevention of cardiovascular disease?\",\n",
    "        \"How does aspirin affect prostaglandin synthesis?\",\n",
    "    ],\n",
    "    'metformin': [\n",
    "        \"How does metformin lower blood glucose in type 2 diabetes?\",\n",
    "        \"What are the contraindications of metformin?\",\n",
    "        \"Does metformin cause lactic acidosis?\",\n",
    "        \"What is the effect of metformin on cardiovascular outcomes?\",\n",
    "        \"How does metformin affect hepatic glucose production?\",\n",
    "    ],\n",
    "    'statins': [\n",
    "        \"How do statins reduce LDL cholesterol?\",\n",
    "        \"Do statins reduce the risk of stroke?\",\n",
    "        \"What is the mechanism of statin-induced myopathy?\",\n",
    "        \"Do statins reduce mortality in patients with heart failure?\",\n",
    "        \"What is the role of statins in primary prevention of cardiovascular disease?\",\n",
    "    ],\n",
    "    'insulin': [\n",
    "        \"How does insulin regulate blood glucose levels?\",\n",
    "        \"What is the mechanism of insulin resistance in type 2 diabetes?\",\n",
    "        \"What are the risks of insulin therapy in type 1 diabetes?\",\n",
    "        \"How does insulin affect lipid metabolism?\",\n",
    "        \"What is the difference between basal and bolus insulin?\",\n",
    "    ],\n",
    "    'ace_inhibitors': [\n",
    "        \"How do ACE inhibitors reduce blood pressure?\",\n",
    "        \"What is the role of ACE inhibitors in heart failure?\",\n",
    "        \"Do ACE inhibitors protect renal function in diabetic nephropathy?\",\n",
    "        \"What are the adverse effects of ACE inhibitors?\",\n",
    "        \"Can ACE inhibitors cause hyperkalemia?\",\n",
    "    ],\n",
    "    'beta_blockers': [\n",
    "        \"How do beta-blockers reduce heart rate?\",\n",
    "        \"Do beta-blockers improve survival after myocardial infarction?\",\n",
    "        \"What are the contraindications of beta-blockers?\",\n",
    "        \"How do beta-blockers affect cardiac output?\",\n",
    "        \"What is the role of beta-blockers in treating hypertension?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "total_questions = sum(len(qs) for qs in CONCEPT_GROUPS.values())\n",
    "print(f'Concept groups: {len(CONCEPT_GROUPS)}')\n",
    "print(f'Total questions: {total_questions}')\n",
    "for concept, qs in CONCEPT_GROUPS.items():\n",
    "    print(f'  {concept}: {len(qs)} questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004",
   "metadata": {},
   "outputs": [],
   "source": "# ── Run Pipeline Across All Models via OpenRouter ─────────────────────────────\n# Each model is called via the OpenRouter key set above.\n# Results are cached per model so re-running skips completed models.\n\nPREFER = 'openrouter'\n\nOPENROUTER_MODELS = {\n    'claude-haiku': 'anthropic/claude-haiku-4-5',          # Anthropic — fast\n    'gpt-4o-mini':  'openai/gpt-4o-mini',                  # OpenAI — cheap\n    'gemini-flash': 'google/gemini-flash-1.5',             # Google — fast\n    'llama-70b':    'meta-llama/llama-3.3-70b-instruct:free',  # Meta — free\n}\n\ndef run_concept_pipeline(question: str, prefer: str = 'openrouter', model: str = None) -> dict:\n    cot = generate_cot(question, prefer=prefer, model=model)\n    steps = cot.get('steps', [])\n    concepts = extract_concepts(steps, scispacy_when='never', top_k=3)\n    return {\n        'question':  question,\n        'model':     cot.get('model', 'unknown'),\n        'model_name': model,\n        'steps':     steps,\n        'concepts':  concepts,\n        'provider':  cot.get('provider', 'unknown'),\n    }\n\ndef _cache_has_umls(results):\n    \"\"\"Check if cached results contain real UMLS concept data.\"\"\"\n    for r in results:\n        for step_concepts in r.get('concepts', []):\n            for c in step_concepts:\n                if c.get('cui') or c.get('valid'):\n                    return True\n    return False\n\n# Run all concept-grouped questions for every model\nall_model_results = {}   # {model_name: [result, ...]}\n\nfor model_name, model_id in OPENROUTER_MODELS.items():\n    cache = RESULTS_DIR / f'exp2_{model_name}_results.json'\n    if cache.exists():\n        with open(cache) as f:\n            cached = json.load(f)\n\n        # Invalidate stale cache if UMLS is now configured but cache has no UMLS data\n        if umls_configured() and not _cache_has_umls(cached):\n            print(f\"  [{model_name}] Cache has NO UMLS data — deleting stale cache to re-run with UMLS\")\n            cache.unlink()\n        else:\n            print(f\"  [{model_name}] Loading from cache ({cache.name}, UMLS: {_cache_has_umls(cached)})\")\n            all_model_results[model_name] = cached\n            continue\n\n    print(f\"  [{model_name}] Running {sum(len(v) for v in CONCEPT_GROUPS.values())} questions...\")\n    results = []\n    for concept, questions in CONCEPT_GROUPS.items():\n        for qi, q in enumerate(questions):\n            try:\n                r = run_concept_pipeline(q, prefer=PREFER, model=model_id)\n                r['concept_group'] = concept\n                results.append(r)\n                print(f\"    {concept} Q{qi+1}: {len(r['steps'])} steps\")\n            except Exception as e:\n                print(f\"    {concept} Q{qi+1}: ERROR {e}\")\n            import time; time.sleep(0.5)\n\n    with open(cache, 'w') as f:\n        json.dump(results, f, indent=2)\n    all_model_results[model_name] = results\n    print(f\"  [{model_name}] Done — {len(results)} results cached.\")\n\n# Use first model as primary for downstream single-model cells\n_primary_model  = next(iter(OPENROUTER_MODELS))\nconcept_results = all_model_results[_primary_model]\n\n# Reconstruct group_results dict keyed by concept (used by downstream cells)\ngroup_results = {}\nfor r in concept_results:\n    group_results.setdefault(r.get('concept_group', 'unknown'), []).append(r)\n\nprint(f\"\\nPrimary model for downstream analysis: {_primary_model}\")\nprint(f\"Models available: {list(all_model_results.keys())}\")\nprint(f\"group_results constructed for concepts: {list(group_results.keys())}\")\nprint(f\"UMLS configured: {umls_configured()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005",
   "metadata": {},
   "outputs": [],
   "source": "# ── Extract Steps Containing Each Concept ─────────────────────────────────────\n# For each concept group, collect all CoT steps and the question they came from.\n# We use simple keyword matching + UMLS CUIs to identify steps mentioning the concept.\n\nfrom utils.hybrid_checker import collect_cuis, jaccard as cui_jaccard\n\nCONCEPT_KEYWORDS = {\n    'aspirin':       ['aspirin', 'acetylsalicylic', 'cox-1', 'cox1', 'thromboxane', 'platelet'],\n    'metformin':     ['metformin', 'biguanide', 'hepatic glucose', 'ampk'],\n    'statins':       ['statin', 'hmg-coa', 'atorvastatin', 'simvastatin', 'lovastatin', 'ldl'],\n    'insulin':       ['insulin', 'pancreatic', 'beta cell', 'glucose uptake', 'glycogen'],\n    'ace_inhibitors': ['ace inhibitor', 'angiotensin', 'ace', 'captopril', 'lisinopril', 'enalapril'],\n    'beta_blockers': ['beta-blocker', 'beta blocker', 'metoprolol', 'atenolol', 'propranolol', 'adrenergic'],\n}\n\ndef step_mentions_concept(step: str, keywords: list) -> bool:\n    step_lower = step.lower()\n    return any(kw in step_lower for kw in keywords)\n\n# Build: {concept: [(q_idx, step_idx, step_text, question, cuis), ...]}\nconcept_steps = defaultdict(list)\n\nfor concept, results in group_results.items():\n    keywords = CONCEPT_KEYWORDS.get(concept, [concept.replace('_', ' ')])\n    for q_idx, r in enumerate(results):\n        steps = r.get('steps', [])\n        concepts_per_step = r.get('concepts', [])\n        for s_idx, step in enumerate(steps):\n            # Collect CUIs from this step's extracted concepts\n            step_concepts = concepts_per_step[s_idx] if s_idx < len(concepts_per_step) else []\n            step_cuis = collect_cuis(step_concepts)\n            concept_steps[concept].append({\n                'q_idx': q_idx,\n                's_idx': s_idx,\n                'step': step,\n                'question': r['question'],\n                'mentions_concept': step_mentions_concept(step, keywords),\n                'cuis': step_cuis,\n                'n_valid_concepts': sum(1 for c in step_concepts if c.get('valid')),\n            })\n\nprint('Step extraction complete (with UMLS CUI tracking):')\nfor concept, steps in concept_steps.items():\n    mentions = sum(1 for s in steps if s['mentions_concept'])\n    with_cuis = sum(1 for s in steps if s['cuis'])\n    print(f'  {concept}: {len(steps)} total steps, {mentions} mention concept, {with_cuis} have CUIs')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006",
   "metadata": {},
   "outputs": [],
   "source": "# ── Cross-Response NLI ────────────────────────────────────────────────────────\n# For each concept group:\n#   - Take pairs of steps from DIFFERENT questions\n#   - Run NLI entailment\n#   - Record contradiction rate + CUI Jaccard overlap\n\nCROSS_NLI_CACHE = RESULTS_DIR / f'exp2_{PREFER}_cross_nli.json'\nMAX_PAIRS_PER_CONCEPT = 50  # cap to avoid too many API calls\n\nif CROSS_NLI_CACHE.exists():\n    print(f'Loading cached cross-NLI from {CROSS_NLI_CACHE}')\n    with open(CROSS_NLI_CACHE) as f:\n        cross_nli_results = json.load(f)\nelse:\n    cross_nli_results = {}\n    \n    for concept, steps_list in concept_steps.items():\n        print(f'\\nCross-NLI for concept: {concept}')\n        \n        # Group steps by question index\n        by_question = defaultdict(list)\n        for s in steps_list:\n            by_question[s['q_idx']].append(s)\n        \n        q_indices = list(by_question.keys())\n        cross_pairs = []\n        \n        # Generate cross-question pairs\n        for i, j in itertools.combinations(q_indices, 2):\n            steps_i = by_question[i]\n            steps_j = by_question[j]\n            for si in steps_i[:2]:\n                for sj in steps_j[:2]:\n                    cross_pairs.append((si, sj))\n                    if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n                        break\n                if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n                    break\n            if len(cross_pairs) >= MAX_PAIRS_PER_CONCEPT:\n                break\n        \n        print(f'  {len(cross_pairs)} cross-question step pairs to evaluate')\n        \n        # Run NLI on all cross pairs\n        concept_results = []\n        for (si, sj) in cross_pairs:\n            try:\n                nli_out = check_entailment([si['step'], sj['step']])\n                if nli_out:\n                    record = nli_out[0]\n                    # Compute CUI Jaccard overlap between cross-question steps\n                    cuis_a = si.get('cuis', [])\n                    cuis_b = sj.get('cuis', [])\n                    umls_jac = cui_jaccard(cuis_a, cuis_b) if cuis_a or cuis_b else 0.0\n                    concept_results.append({\n                        'q_idx_a': si['q_idx'],\n                        'q_idx_b': sj['q_idx'],\n                        'question_a': si['question'],\n                        'question_b': sj['question'],\n                        'step_a': si['step'],\n                        'step_b': sj['step'],\n                        'label': record.get('label', 'unknown'),\n                        'probs': record.get('probs', {}),\n                        'same_question': si['q_idx'] == sj['q_idx'],\n                        'umls_jaccard': umls_jac,\n                        'cuis_shared': list(set(cuis_a) & set(cuis_b)),\n                    })\n            except Exception as e:\n                pass\n        \n        cross_nli_results[concept] = concept_results\n        ok = sum(1 for r in concept_results if r['label'] != 'unknown')\n        cui_pairs = sum(1 for r in concept_results if r.get('umls_jaccard', 0) > 0)\n        print(f'  {ok}/{len(concept_results)} pairs scored, {cui_pairs} with CUI overlap')\n    \n    with open(CROSS_NLI_CACHE, 'w') as f:\n        json.dump(cross_nli_results, f, indent=2, ensure_ascii=False)\n    print(f'\\nCached to {CROSS_NLI_CACHE}')\n\nprint('\\nCross-NLI results loaded for all concepts.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007",
   "metadata": {},
   "outputs": [],
   "source": "# ── Compute Cross-Response Contradiction Score ─────────────────────────────────\n\nrows = []\nfor concept, results in cross_nli_results.items():\n    for r in results:\n        rows.append({\n            'concept': concept,\n            'q_idx_a': r.get('q_idx_a'),\n            'q_idx_b': r.get('q_idx_b'),\n            'question_a': r.get('question_a', '')[:60],\n            'question_b': r.get('question_b', '')[:60],\n            'step_a': r.get('step_a', '')[:80],\n            'step_b': r.get('step_b', '')[:80],\n            'label': r.get('label', 'unknown'),\n            'prob_contradiction': (r.get('probs') or {}).get('contradiction', 0),\n            'prob_entailment': (r.get('probs') or {}).get('entailment', 0),\n            'prob_neutral': (r.get('probs') or {}).get('neutral', 0),\n            'same_question': r.get('same_question', False),\n            'umls_jaccard': r.get('umls_jaccard', 0.0),\n            'n_shared_cuis': len(r.get('cuis_shared', [])),\n        })\n\ndf_cross = pd.DataFrame(rows)\n\n# Per-concept cross-response contradiction rate\nconcept_stats = df_cross[~df_cross['same_question']].groupby('concept').agg(\n    n_pairs          = ('label', 'count'),\n    contradiction_rate = ('label', lambda x: (x == 'contradiction').mean()),\n    entailment_rate  = ('label', lambda x: (x == 'entailment').mean()),\n    avg_p_contra     = ('prob_contradiction', 'mean'),\n    avg_p_entail     = ('prob_entailment', 'mean'),\n    avg_umls_jaccard = ('umls_jaccard', 'mean'),\n    pairs_with_cui_overlap = ('n_shared_cuis', lambda x: (x > 0).sum()),\n).round(4).sort_values('contradiction_rate', ascending=False)\n\nprint('=== Cross-Response Contradiction Rates by Concept (with UMLS CUI overlap) ===\\n')\nprint(concept_stats.to_string())\nconcept_stats.to_csv(RESULTS_DIR / 'exp2_concept_stats.csv')\n\n# Additional insight: CUI overlap vs contradiction\ncross_only = df_cross[~df_cross['same_question']]\nif len(cross_only) > 0:\n    has_cui = cross_only['umls_jaccard'] > 0\n    contra_with_cui = (cross_only[has_cui]['label'] == 'contradiction').mean() if has_cui.sum() > 0 else float('nan')\n    contra_without_cui = (cross_only[~has_cui]['label'] == 'contradiction').mean() if (~has_cui).sum() > 0 else float('nan')\n    print(f'\\nContradiction rate with shared CUIs:    {contra_with_cui:.4f} (n={has_cui.sum()})')\n    print(f'Contradiction rate without shared CUIs: {contra_without_cui:.4f} (n={(~has_cui).sum()})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 1: Cross-Response vs. Within-Response Contradiction Rate ────────────\n",
    "\n",
    "# Also compute within-response contradiction rates for comparison\n",
    "within_rows = []\n",
    "for concept, results in group_results.items():\n",
    "    for r in results:\n",
    "        steps = r.get('steps', [])\n",
    "        concepts = r.get('concepts', [])\n",
    "        if len(steps) < 2:\n",
    "            continue\n",
    "        pairs = build_entailment_records(steps, concepts)\n",
    "        for p in pairs:\n",
    "            within_rows.append({\n",
    "                'concept': concept,\n",
    "                'label': p.get('final_label', 'unknown'),\n",
    "                'prob_contradiction': p.get('probs', {}).get('contradiction', 0),\n",
    "            })\n",
    "\n",
    "df_within = pd.DataFrame(within_rows)\n",
    "within_stats = df_within.groupby('concept').agg(\n",
    "    contradiction_rate = ('label', lambda x: (x == 'contradiction').mean())\n",
    ").round(4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# (a) Cross-response contradiction rate per concept\n",
    "ax = axes[0]\n",
    "cross_sorted = concept_stats['contradiction_rate'].sort_values(ascending=False)\n",
    "bars = ax.bar(range(len(cross_sorted)), cross_sorted.values, color='#C44E52', alpha=0.85)\n",
    "ax.set_xticks(range(len(cross_sorted)))\n",
    "ax.set_xticklabels([c.replace('_', ' ') for c in cross_sorted.index], rotation=30, ha='right')\n",
    "ax.set_ylabel('Cross-Response Contradiction Rate')\n",
    "ax.set_title('(a) Cross-Question Contradiction Rate by Concept')\n",
    "\n",
    "# (b) Within vs. cross comparison\n",
    "ax = axes[1]\n",
    "concepts_common = list(set(cross_sorted.index) & set(within_stats.index))\n",
    "x = np.arange(len(concepts_common))\n",
    "w = 0.35\n",
    "ax.bar(x - w/2, [within_stats.loc[c, 'contradiction_rate'] for c in concepts_common],\n",
    "       w, label='Within-answer', color='#4C72B0', alpha=0.85)\n",
    "ax.bar(x + w/2, [cross_sorted.get(c, 0) for c in concepts_common],\n",
    "       w, label='Cross-answer', color='#C44E52', alpha=0.85)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([c.replace('_', ' ') for c in concepts_common], rotation=30, ha='right')\n",
    "ax.set_ylabel('Contradiction Rate')\n",
    "ax.set_title('(b) Within-Answer vs. Cross-Answer Contradictions')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('Experiment 2: Cross-Question Ontological Consistency', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp2_fig1_cross_vs_within.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 1 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 2: Cross-Question Contradiction Heatmap ────────────────────────────\n",
    "# For each concept group, show the pairwise contradiction rate between question pairs\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, (concept, results) in enumerate(group_results.items()):\n",
    "    if ax_idx >= len(axes):\n",
    "        break\n",
    "    ax = axes[ax_idx]\n",
    "    \n",
    "    n_q = len(results)\n",
    "    matrix = np.zeros((n_q, n_q))\n",
    "    count_matrix = np.zeros((n_q, n_q))\n",
    "    \n",
    "    cross_data = df_cross[df_cross['concept'] == concept]\n",
    "    for _, row in cross_data.iterrows():\n",
    "        qi, qj = int(row.get('q_idx_a', 0)), int(row.get('q_idx_b', 0))\n",
    "        if qi < n_q and qj < n_q:\n",
    "            val = 1.0 if row['label'] == 'contradiction' else 0.0\n",
    "            matrix[qi][qj] += val\n",
    "            matrix[qj][qi] += val\n",
    "            count_matrix[qi][qj] += 1\n",
    "            count_matrix[qj][qi] += 1\n",
    "    \n",
    "    # Normalize\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        norm_matrix = np.where(count_matrix > 0, matrix / count_matrix, np.nan)\n",
    "    \n",
    "    im = ax.imshow(norm_matrix, cmap='Reds', vmin=0, vmax=1, aspect='auto')\n",
    "    ax.set_title(f'{concept.replace(\"_\", \" \").title()}')\n",
    "    ax.set_xlabel('Question index')\n",
    "    ax.set_ylabel('Question index')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle('Cross-Question Contradiction Rate Heatmaps per Concept', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'exp2_fig2_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Figure 2 saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Top Cross-Response Contradiction Examples ──────────────────────────────────\n",
    "\n",
    "print('=== Top Cross-Response Contradiction Examples ===\\n')\n",
    "\n",
    "top = df_cross[(df_cross['label'] == 'contradiction') & (~df_cross['same_question'])]\\\n",
    "    .nlargest(8, 'prob_contradiction')\n",
    "\n",
    "for _, row in top.iterrows():\n",
    "    print(f\"Concept: {row['concept']} | P(contra): {row['prob_contradiction']:.3f}\")\n",
    "    print(f\"  Q1: {row['question_a']}\")\n",
    "    print(f\"  Step A: {row['step_a']}\")\n",
    "    print(f\"  Q2: {row['question_b']}\")\n",
    "    print(f\"  Step B: {row['step_b']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Statistical Test: Is Cross-Answer Rate > Within-Answer Rate? ──────────────\n",
    "\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "\n",
    "within_rates = []\n",
    "cross_rates  = []\n",
    "\n",
    "for concept in CONCEPT_GROUPS.keys():\n",
    "    if concept in within_stats.index:\n",
    "        within_rates.append(float(within_stats.loc[concept, 'contradiction_rate']))\n",
    "    if concept in concept_stats.index:\n",
    "        cross_rates.append(float(concept_stats.loc[concept, 'contradiction_rate']))\n",
    "\n",
    "print('=== Statistical Comparison: Cross-Answer vs. Within-Answer ===\\n')\n",
    "print(f'Within-answer contradiction rates: {[round(x, 3) for x in within_rates]}')\n",
    "print(f'Cross-answer  contradiction rates: {[round(x, 3) for x in cross_rates]}')\n",
    "print(f'Mean within: {np.mean(within_rates):.4f}')\n",
    "print(f'Mean cross:  {np.mean(cross_rates):.4f}')\n",
    "\n",
    "if len(within_rates) >= 3 and len(cross_rates) >= 3:\n",
    "    try:\n",
    "        stat, p = mannwhitneyu(cross_rates, within_rates, alternative='greater')\n",
    "        print(f'\\nMann-Whitney U test (cross > within):')\n",
    "        print(f'  U={stat:.2f}, p={p:.4f}')\n",
    "        if p < 0.05:\n",
    "            print('  ✓ Cross-answer contradiction rate is SIGNIFICANTLY HIGHER than within-answer.')\n",
    "        else:\n",
    "            print('  ✗ No significant difference detected (may need more data).')\n",
    "    except Exception as e:\n",
    "        print(f'  Test failed: {e}')\n",
    "\n",
    "print('\\nKey finding: Even when within-answer reasoning is locally consistent,')\n",
    "print('LLMs may assert contradictory claims about the same concept across questions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "1. **Cross-response contradictions are real** — LLMs assert conflicting claims about the same medical concept across different questions\n",
    "2. **Concept-level variation** — some concepts (e.g., drug interactions) show higher cross-response contradiction rates than others\n",
    "3. **Heatmap pattern** — certain question pairs are consistently contradictory, suggesting specific factual inconsistencies\n",
    "\n",
    "**For the paper:**\n",
    "- Table 1 → concept_stats DataFrame  \n",
    "- Figure 1 → within vs. cross comparison\n",
    "- Figure 2 → heatmaps\n",
    "- Section 3 → describe the cross-response NLI method\n",
    "- Section 4 → statistical comparison\n",
    "\n",
    "**Implication for workshop:** Addresses the ICLR 2026 topic *\"Avoiding Logical Contradictions Across Responses to Multiple Related Questions\"* directly."
   ]
  }
 ]
}