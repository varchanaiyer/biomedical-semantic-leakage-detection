{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "test-01",
   "metadata": {},
   "source": "# Pipeline End-to-End Test Notebook\n\nThis notebook walks through **every stage** of the Biomedical Semantic Leakage Detection pipeline interactively, showing real output at each step.\n\n| Stage | What it tests |\n|---|---|\n| 0 | Environment setup & imports |\n| 1 | CoT Generator \u2014 live LLM call |\n| 2 | Concept Extractor \u2014 UMLS concept linking |\n| 3 | Hybrid NLI Entailment Checker |\n| 4 | Guard Signal Derivation |\n| 5 | Full Pipeline (3 questions end-to-end) |\n| 6 | Results Summary & Visualisation |\n\nRun cells top-to-bottom. Each cell is independent and shows what it produced."
  },
  {
   "cell_type": "markdown",
   "id": "test-02",
   "metadata": {},
   "source": "## Stage 0 \u2014 Environment Setup\n\nClones the repo, installs dependencies, configures API keys. **Run this first.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 0a. Clone / pull the repo and configure paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os, sys\nfrom pathlib import Path\n\nREPO_URL  = 'https://github.com/varchanaiyer/biomedical-semantic-leakage-detection'\nREPO_DIR  = 'biomedical-semantic-leakage-detection'\n\nif not Path(REPO_DIR).exists():\n    os.system(f'git clone {REPO_URL}')\nelse:\n    os.system(f'git -C {REPO_DIR} pull --quiet')\n\n_cwd = Path(os.getcwd())\nif (_cwd / REPO_DIR / 'utils').exists():\n    PROJECT_ROOT = str(_cwd / REPO_DIR)\nelif (_cwd / 'utils').exists():\n    PROJECT_ROOT = str(_cwd)\nelif (_cwd.parent / 'utils').exists():\n    PROJECT_ROOT = str(_cwd.parent)\nelse:\n    PROJECT_ROOT = str(_cwd / REPO_DIR)\n\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\nos.chdir(PROJECT_ROOT)\n\nprint(f\"PROJECT_ROOT : {PROJECT_ROOT}\")\nprint(f\"Python       : {sys.version.split()[0]}\")\nprint(f\"Working dir  : {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \u2500\u2500 0b. Install dependencies \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n!pip install openai numpy pandas scipy scikit-learn matplotlib seaborn requests --quiet ipywidgets\nprint(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-05",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \u2500\u2500 OpenRouter API Key \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os, importlib.util\nfrom IPython.display import display, clear_output, HTML\n\n_HAS_WIDGETS = importlib.util.find_spec(\"ipywidgets\") is not None\n\nif _HAS_WIDGETS:\n    import ipywidgets as widgets\n\n    _key_box = widgets.Password(\n        placeholder=\"sk-or-v1-\u2026  (get yours free at openrouter.ai)\",\n        layout=widgets.Layout(width=\"520px\"),\n    )\n    _btn = widgets.Button(\n        description=\"Set Key\", button_style=\"primary\",\n        icon=\"check\", layout=widgets.Layout(width=\"110px\"),\n    )\n    _out = widgets.Output()\n\n    def _apply(_b):\n        with _out:\n            clear_output()\n            key = _key_box.value.strip()\n            if key:\n                os.environ[\"OPENROUTER_API_KEY\"] = key\n                print(f\"  \u2713 OpenRouter key set ({len(key)} chars)\")\n            else:\n                print(\"  \u2717 Paste your OpenRouter key above, then click Set Key.\")\n\n    _btn.on_click(_apply)\n    display(HTML(\"<b>\ud83d\udd11 OpenRouter API Key</b>\"))\n    display(widgets.HBox([_key_box, _btn]))\n    display(_out)\n    display(HTML(\n        \"<small>Get a free key at \"\n        \"<a href=\\\"https://openrouter.ai\\\" target=\\\"_blank\\\">openrouter.ai</a>\"\n        \" \u2014 the notebooks will automatically run across all configured models.</small>\"\n    ))\nelse:\n    os.environ.setdefault(\"OPENROUTER_API_KEY\", \"\")\n    print(\"ipywidgets not found \u2014 set key with:\")\n    print(\"  os.environ[\\\"OPENROUTER_API_KEY\\\"] = \\\"sk-or-v1-...\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-06",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 0d. Import all pipeline modules \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport warnings, json, time, traceback\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\n\n_ok = {}\nfor mod, sym in [\n    ('utils.cot_generator',    'generate'),\n    ('utils.concept_extractor','extract_concepts'),\n    ('utils.hybrid_checker',   'build_entailment_records'),\n    ('utils.guards',           'derive_guards'),\n    ('utils.umls_api_linker',  'is_configured'),\n]:\n    try:\n        m = __import__(mod, fromlist=[sym])\n        _ok[mod] = True\n        print(f\"  \u2713 {mod}\")\n    except Exception as e:\n        _ok[mod] = False\n        print(f\"  \u2717 {mod}: {e}\")\n\nfrom utils.cot_generator    import generate as generate_cot, OPENROUTER_READY, ANTHROPIC_READY\nfrom utils.concept_extractor import extract_concepts\nfrom utils.hybrid_checker    import build_entailment_records\nfrom utils.guards            import derive_guards, GuardConfig, lexical_jaccard\nfrom utils.umls_api_linker   import is_configured as umls_configured\n\nGUARD_CFG = GuardConfig()\n\nprint()\nprint(f\"  OpenRouter ready : {OPENROUTER_READY}\")\nprint(f\"  Anthropic ready  : {ANTHROPIC_READY}\")\nprint(f\"  UMLS configured  : {umls_configured()}\")\nprint(f\"  Heuristic NLI    : {os.environ.get('FORCE_HEURISTIC_NLI') == '1'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-07",
   "metadata": {},
   "source": "## Stage 1 \u2014 CoT Generator\n\nCalls an LLM to produce numbered reasoning steps for a biomedical question.  \nIf no API key is set, falls back to 5 generic template steps (provider = `local`)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 1a. Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Change these to try different models\nPREFER = 'openrouter'\nMODEL  = 'anthropic/claude-haiku-4-5'   # any OpenRouter slug\n# MODEL = 'openai/gpt-4o-mini'\n# MODEL = 'google/gemini-flash-1.5'\n# MODEL = 'meta-llama/llama-3.3-70b-instruct'\n\nTEST_QUESTION = (\n    \"Does aspirin reduce the risk of myocardial infarction \"\n    \"in patients with cardiovascular disease?\"\n)\nprint(f\"Question : {TEST_QUESTION}\")\nprint(f\"Model    : {MODEL} via {PREFER}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-09",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 1b. Run CoT generation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nt0  = time.time()\ncot = generate_cot(TEST_QUESTION, prefer=PREFER, model=MODEL)\nelapsed = round(time.time() - t0, 2)\n\nSTEPS    = cot['steps']\nPROVIDER = cot['provider']\nMODEL_ID = cot['model']\n\nprint(f\"Provider : {PROVIDER}  |  Model : {MODEL_ID}  |  Time : {elapsed}s\")\nprint(f\"Steps    : {len(STEPS)}\")\nprint()\n\n# Display steps\nfor i, step in enumerate(STEPS, 1):\n    print(f\"  Step {i:2d}: {step}\")\n\n# Warnings\nif PROVIDER == 'local':\n    print()\n    print(\"\u26a0  provider='local' means all API calls failed.\")\n    print(\"   Set OPENROUTER_API_KEY in cell 0c and re-run.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-10",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 1c. Validate step quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nchecks = {\n    'At least 3 steps returned':         len(STEPS) >= 3,\n    'All steps non-empty strings':        all(isinstance(s, str) and len(s.strip()) > 0 for s in STEPS),\n    'All steps > 15 chars (not trivial)': all(len(s) > 15 for s in STEPS),\n    'Real LLM was called (not fallback)': PROVIDER != 'local',\n}\n\nall_pass = True\nfor name, result in checks.items():\n    icon = '\u2713' if result else '\u2717'\n    print(f\"  {icon}  {name}\")\n    if not result: all_pass = False\n\nprint()\nprint(\"Stage 1:\", \"PASS \u2713\" if all_pass else \"WARN \u2014 check API key\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-11",
   "metadata": {},
   "source": "## Stage 2 \u2014 Concept Extractor\n\nExtracts biomedical surface candidates (n-grams, acronyms) from each step  \nand links them to UMLS concepts (CUIs) if the UMLS API is configured."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-12",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 2a. Extract concepts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nt0       = time.time()\nCONCEPTS = extract_concepts(STEPS, scispacy_when='never', top_k=5)\nelapsed  = round(time.time() - t0, 2)\n\ntotal_cands = sum(len(c) for c in CONCEPTS)\nvalid_cands = sum(1 for sc in CONCEPTS for c in sc if c.get('valid'))\n\nprint(f\"Elapsed         : {elapsed}s\")\nprint(f\"Total candidates: {total_cands}  (across {len(STEPS)} steps)\")\nprint(f\"Valid (UMLS CUI): {valid_cands}\")\nprint(f\"UMLS configured : {umls_configured()}\")\nif not umls_configured():\n    print()\n    print(\"  \u2139  UMLS not configured \u2014 concept candidates will have no CUI.\")\n    print(\"     Set UMLS_API_KEY in cell 0c for full concept linking.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 2b. Show concepts per step \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrows = []\nfor i, (step, cands) in enumerate(zip(STEPS, CONCEPTS)):\n    for c in cands[:3]:   # top-3 per step\n        rows.append({\n            'Step':       i + 1,\n            'Step text':  step[:55] + '...' if len(step) > 55 else step,\n            'Surface':    c.get('surface', '?'),\n            'Name':       c.get('name') or c.get('surface', '?'),\n            'CUI':        c.get('cui', '\u2014'),\n            'Confidence': round(float((c.get('scores') or {}).get('confidence', 0)), 3),\n            'Valid':      c.get('valid', False),\n        })\n\nif rows:\n    df_concepts = pd.DataFrame(rows)\n    display(df_concepts.to_string(index=False))\nelse:\n    print(\"No concept candidates returned.\")\n    print(\"This is expected when UMLS_API_KEY is not set.\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-14",
   "metadata": {},
   "source": "## Stage 3 \u2014 Hybrid NLI Entailment Checker\n\nScores each adjacent step-pair for entailment / neutral / contradiction.\n\n- With `FORCE_HEURISTIC_NLI=1` (default): uses token-overlap heuristic (fast, no download)\n- With `FORCE_HEURISTIC_NLI=0`: downloads and runs PubMedBERT-BioNLI-LoRA (~420MB)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-15",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 3a. Run NLI on step pairs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nt0    = time.time()\nPAIRS = build_entailment_records(STEPS, CONCEPTS)\nelapsed = round(time.time() - t0, 2)\n\nlabel_counts = Counter(p['final_label'] for p in PAIRS)\nprint(f\"Elapsed        : {elapsed}s\")\nprint(f\"Adjacent pairs : {len(PAIRS)}  (= {len(STEPS)} steps - 1)\")\nprint(f\"Label counts   : {dict(label_counts)}\")\nprint(f\"NLI source     : {(PAIRS[0].get('meta') or {}).get('nli_source', '?') if PAIRS else 'n/a'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-16",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 3b. Display pair probabilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrows = []\nfor p in PAIRS:\n    i, j   = p['step_pair']\n    probs  = p.get('probs', {})\n    rows.append({\n        'Pair':          f'{i}\u2192{j}',\n        'Premise':       STEPS[i][:50] + '...' if len(STEPS[i]) > 50 else STEPS[i],\n        'Hypothesis':    STEPS[j][:50] + '...' if len(STEPS[j]) > 50 else STEPS[j],\n        'P(entail)':     round(probs.get('entailment',   0), 3),\n        'P(neutral)':    round(probs.get('neutral',      0), 3),\n        'P(contra)':     round(probs.get('contradiction',0), 3),\n        'Final label':   p.get('final_label', '?'),\n    })\n\ndf_nli = pd.DataFrame(rows)\n\n# Colour the Final label column\ndef colour_label(val):\n    colours = {'contradiction':'#ffcccc','entailment':'#ccffcc','neutral':'#e8e8e8'}\n    return f\"background-color: {colours.get(val, 'white')}\"\n\ndisplay(df_nli.style.applymap(colour_label, subset=['Final label'])\n               .format({'P(entail)':'{:.3f}','P(neutral)':'{:.3f}','P(contra)':'{:.3f}'}))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-17",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 3c. Validate NLI output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nvalid_labels = {'entailment', 'neutral', 'contradiction'}\nchecks = {\n    f'Returns {len(STEPS)-1} pairs (N-1)':  len(PAIRS) == len(STEPS) - 1,\n    'All probs sum to \u2248 1.0':\n        all(abs(sum(p['probs'].values()) - 1.0) < 0.05 for p in PAIRS),\n    'All final_labels are valid':\n        all(p.get('final_label') in valid_labels for p in PAIRS),\n}\nfor name, ok in checks.items():\n    print(f\"  {'\u2713' if ok else '\u2717'}  {name}\")\nprint()\nprint(\"Stage 3:\", \"PASS \u2713\" if all(checks.values()) else \"FAIL \u2717\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-18",
   "metadata": {},
   "source": "## Stage 4 \u2014 Guard Signal Derivation\n\nGuard signals are qualitative tags computed **on top of** NLI probabilities.\n\n| Guard | Fires when |\n|---|---|\n| `lexical_duplicate` | Adjacent steps are \u2265 90% lexically identical (wasted reasoning) |\n| `caution_band` | Top two label probabilities are very close (the model is uncertain) |\n| `direction_conflict` | NLI is asymmetric: A\u2192B entails but B\u2192A contradicts (requires bidirectional NLI) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-19",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 4a. Compute guard signals for each pair \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef _reverse_probs(steps, concepts, i, j):\n    try:\n        rev = build_entailment_records([steps[j], steps[i]],\n                                       [concepts[j] if j < len(concepts) else [],\n                                        concepts[i] if i < len(concepts) else []])\n        return rev[0]['probs'] if rev else None\n    except Exception:\n        return None\n\nGUARDED_PAIRS = []\nfor p in PAIRS:\n    i, j      = p['step_pair']\n    rev_probs = _reverse_probs(STEPS, CONCEPTS, i, j)\n    guards    = derive_guards(\n        premise       = STEPS[i] if i < len(STEPS) else '',\n        hypothesis    = STEPS[j] if j < len(STEPS) else '',\n        probs         = p['probs'],\n        reverse_probs = rev_probs,\n        config        = GUARD_CFG,\n    )\n    GUARDED_PAIRS.append({**p, 'guards': guards, 'reverse_probs': rev_probs})\n\nall_guards = [g for p in GUARDED_PAIRS for g in p['guards']]\nprint(f\"Total guards fired : {len(all_guards)}\")\nprint(f\"Guard breakdown    : {dict(Counter(all_guards)) or 'none'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-20",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 4b. Display guard signals per pair \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nrows = []\nfor p in GUARDED_PAIRS:\n    i, j   = p['step_pair']\n    probs  = p['probs']\n    rprobs = p.get('reverse_probs') or {}\n    rows.append({\n        'Pair':           f'{i}\u2192{j}',\n        'Label':          p['final_label'],\n        'P(contra) fwd':  round(probs.get('contradiction', 0), 3),\n        'P(entail) rev':  round(rprobs.get('entailment', 0), 3) if rprobs else '\u2014',\n        'Guards':         ', '.join(p['guards']) or 'none',\n    })\n\ndf_guards = pd.DataFrame(rows)\ndisplay(df_guards.to_string(index=False))\n\nprint()\nprint(\"Guard explanations:\")\nprint(\"  caution_band      \u2014 uncertain; top 2 NLI labels are very close in probability\")\nprint(\"  lexical_duplicate \u2014 steps are nearly identical; probably not advancing reasoning\")\nprint(\"  direction_conflict\u2014 A entails B but B contradicts A (asymmetric relationship)\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-21",
   "metadata": {},
   "source": "## Stage 5 \u2014 Full Pipeline (3 Questions)\n\nRuns all four stages in sequence for 3 different biomedical questions  \nand collects structured output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-22",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 5a. Define 3 test questions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPIPELINE_QUESTIONS = [\n    \"Does aspirin reduce the risk of myocardial infarction in patients with cardiovascular disease?\",\n    \"What is the mechanism by which metformin lowers blood glucose in type 2 diabetes?\",\n    \"How do statins reduce LDL cholesterol and lower cardiovascular risk?\",\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 5b. Run the full pipeline on each question \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndef run_pipeline(question, prefer=PREFER, model=MODEL):\n    t0 = time.time()\n    cot      = generate_cot(question, prefer=prefer, model=model)\n    steps    = cot.get('steps', [])\n    concepts = extract_concepts(steps, scispacy_when='never', top_k=3)\n    pairs    = build_entailment_records(steps, concepts)\n\n    guarded = []\n    for p in pairs:\n        i, j      = p['step_pair']\n        rev_probs = _reverse_probs(steps, concepts, i, j)\n        guards    = derive_guards(\n            premise=steps[i] if i < len(steps) else '',\n            hypothesis=steps[j] if j < len(steps) else '',\n            probs=p['probs'],\n            reverse_probs=rev_probs,\n            config=GUARD_CFG,\n        )\n        guarded.append({**p, 'guards': guards})\n\n    return {\n        'question':  question,\n        'provider':  cot.get('provider'),\n        'model':     cot.get('model'),\n        'steps':     steps,\n        'concepts':  concepts,\n        'pairs':     guarded,\n        'elapsed_s': round(time.time() - t0, 2),\n    }\n\nPIPELINE_RESULTS = []\nfor qi, q in enumerate(PIPELINE_QUESTIONS, 1):\n    print(f\"Q{qi}: {q[:65]}...\")\n    try:\n        r = run_pipeline(q)\n        n_contra = sum(1 for p in r['pairs'] if p['final_label'] == 'contradiction')\n        guards   = [g for p in r['pairs'] for g in p['guards']]\n        print(f\"     provider={r['provider']}  steps={len(r['steps'])}  \"\n              f\"pairs={len(r['pairs'])}  contradictions={n_contra}  \"\n              f\"guards={dict(Counter(guards)) or 'none'}  ({r['elapsed_s']}s)\")\n        PIPELINE_RESULTS.append(r)\n    except Exception as e:\n        print(f\"     ERROR: {e}\")\n        traceback.print_exc()\n    time.sleep(0.5)\n\nprint(f\"\\nCompleted {len(PIPELINE_RESULTS)}/{len(PIPELINE_QUESTIONS)} questions.\")"
  },
  {
   "cell_type": "markdown",
   "id": "test-24",
   "metadata": {},
   "source": "## Stage 6 \u2014 Results Summary & Visualisation\n\nAggregates all pipeline results into a summary table and plots."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 6a. Build summary table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsummary_rows = []\nfor r in PIPELINE_RESULTS:\n    pairs    = r['pairs']\n    labels   = Counter(p['final_label'] for p in pairs)\n    guards   = Counter(g for p in pairs for g in p['guards'])\n    n_pairs  = len(pairs)\n\n    concepts_flat = [c for sc in r['concepts'] for c in sc]\n    valid_concepts = sum(1 for c in concepts_flat if c.get('valid'))\n\n    summary_rows.append({\n        'Question':            r['question'][:60] + '...',\n        'Provider':            r['provider'],\n        'Model':               r['model'],\n        'Steps':               len(r['steps']),\n        'Pairs':               n_pairs,\n        'Entailment':          labels.get('entailment', 0),\n        'Neutral':             labels.get('neutral', 0),\n        'Contradiction':       labels.get('contradiction', 0),\n        'Contradiction Rate':  round(labels.get('contradiction', 0) / n_pairs, 3) if n_pairs else 0,\n        'UMLS Concepts':       len(concepts_flat),\n        'Valid Concepts':      valid_concepts,\n        'caution_band':        guards.get('caution_band', 0),\n        'lexical_duplicate':   guards.get('lexical_duplicate', 0),\n        'direction_conflict':  guards.get('direction_conflict', 0),\n        'Time (s)':            r['elapsed_s'],\n    })\n\ndf_summary = pd.DataFrame(summary_rows)\ndisplay(df_summary.T)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-26",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 6b. Contradiction details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"=== Detected Contradictions ===\\n\")\nany_found = False\nfor r in PIPELINE_RESULTS:\n    for p in r['pairs']:\n        if p['final_label'] == 'contradiction':\n            i, j   = p['step_pair']\n            probs  = p['probs']\n            any_found = True\n            print(f\"Question : {r['question'][:70]}\")\n            print(f\"Pair     : step {i} \u2192 step {j}\")\n            print(f\"  Step {i}: {r['steps'][i]}\")\n            print(f\"  Step {j}: {r['steps'][j]}\")\n            print(f\"  P(contradiction)={probs.get('contradiction',0):.3f}  \"\n                  f\"P(entailment)={probs.get('entailment',0):.3f}  \"\n                  f\"P(neutral)={probs.get('neutral',0):.3f}\")\n            print(f\"  Guards: {p['guards'] or 'none'}\")\n            print()\n\nif not any_found:\n    print(\"No contradictions detected in these 3 questions.\")\n    print(\"Try running with FORCE_HEURISTIC_NLI=0 (real transformer model)\")\n    print(\"or try more adversarial questions.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-27",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 6c. NLI probability heatmap across steps \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, axes = plt.subplots(1, len(PIPELINE_RESULTS), figsize=(5 * len(PIPELINE_RESULTS), 4))\nif len(PIPELINE_RESULTS) == 1:\n    axes = [axes]\n\nfor ax, r in zip(axes, PIPELINE_RESULTS):\n    pairs  = r['pairs']\n    labels = [p['final_label'] for p in pairs]\n    n      = len(r['steps'])\n\n    # Build NxN probability matrix for P(contradiction)\n    mat = np.full((n, n), np.nan)\n    for p in pairs:\n        i, j = p['step_pair']\n        mat[i, j] = p['probs'].get('contradiction', 0)\n\n    im = ax.imshow(mat, vmin=0, vmax=1, cmap='RdYlGn_r', aspect='auto')\n    ax.set_title(f\"{r['question'][:40]}...\", fontsize=8)\n    ax.set_xlabel('Hypothesis step')\n    ax.set_ylabel('Premise step')\n    ax.set_xticks(range(n)); ax.set_yticks(range(n))\n    ax.set_xticklabels(range(1, n+1)); ax.set_yticklabels(range(1, n+1))\n\n    for p in pairs:\n        i, j  = p['step_pair']\n        val   = mat[i, j]\n        label = p['final_label'][0].upper()  # E / N / C\n        color = 'white' if val > 0.5 else 'black'\n        ax.text(j, i, f\"{label}\\n{val:.2f}\", ha='center', va='center',\n                fontsize=7, color=color)\n\nplt.colorbar(im, ax=axes[-1], label='P(contradiction)')\nplt.suptitle('P(contradiction) Heatmap per Step-Pair', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-28",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 6d. Summary bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfig, ax = plt.subplots(figsize=(max(6, len(PIPELINE_RESULTS) * 3), 4))\nx      = np.arange(len(PIPELINE_RESULTS))\nwidth  = 0.25\nlabels = ['Entailment', 'Neutral', 'Contradiction']\ncolors = ['#4C72B0', '#8172B2', '#C44E52']\nkeys   = ['Entailment', 'Neutral', 'Contradiction']\n\nfor ki, (key, color) in enumerate(zip(keys, colors)):\n    vals = [row[key] for row in summary_rows]\n    ax.bar(x + ki * width, vals, width, label=key, color=color, alpha=0.85)\n\nax.set_xticks(x + width)\nax.set_xticklabels(\n    [r['question'][:30] + '...' for r in PIPELINE_RESULTS],\n    fontsize=8, rotation=10, ha='right'\n)\nax.set_ylabel('# Pairs')\nax.set_title('NLI Label Distribution per Question')\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-29",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "# \u2500\u2500 6e. Final pass/fail summary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nprint(\"=\" * 60)\nprint(\"PIPELINE TEST SUMMARY\")\nprint(\"=\" * 60)\n\ntotal = len(PIPELINE_QUESTIONS)\nok    = len(PIPELINE_RESULTS)\npassed = [\n    ('All questions completed',     ok == total,                 f'{ok}/{total}'),\n    ('All used real LLM (not local)',\n     all(r['provider'] != 'local' for r in PIPELINE_RESULTS),\n     ', '.join(r['provider'] for r in PIPELINE_RESULTS)),\n    ('All returned \u2265 3 steps',\n     all(len(r['steps']) >= 3 for r in PIPELINE_RESULTS),\n     ', '.join(str(len(r['steps'])) for r in PIPELINE_RESULTS)),\n    ('All NLI pairs valid',\n     all(p.get('final_label') in {'entailment','neutral','contradiction'}\n         for r in PIPELINE_RESULTS for p in r['pairs']),\n     'ok'),\n    ('Guard signals computed',\n     all('guards' in p for r in PIPELINE_RESULTS for p in r['pairs']),\n     'ok'),\n]\n\nall_ok = True\nfor name, result, detail in passed:\n    icon = '\u2713' if result else '\u2717'\n    print(f\"  {icon}  {name:<40s}  {detail}\")\n    if not result: all_ok = False\n\nprint()\nprint(\"Overall:\", \"\u2713 ALL PASS\" if all_ok else \"\u2717 SOME CHECKS FAILED\")\nprint()\nif not all_ok:\n    print(\"If provider='local': set OPENROUTER_API_KEY in Stage 0 and re-run.\")\n    print(\"If steps < 3: API likely returned an error \u2014 check key and model slug.\")"
  }
 ]
}